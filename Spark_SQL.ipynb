{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["_zJeZZqpJu73","LccNQX8WMBtl","4xTFXrXNSG_b","vw6VZXSpSNZA"],"authorship_tag":"ABX9TyNFzE4V6asI5mWfMfnmxvbU"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# **Spark SQL**"],"metadata":{"id":"IhqAQ_OOKTXl"}},{"cell_type":"markdown","source":["## Introducción"],"metadata":{"id":"_zJeZZqpJu73"}},{"cell_type":"markdown","source":["### `Ventajas y desventajas de trabajar con Spark en Google Colab`"],"metadata":{"id":"Ojkkv5J5rwg-"}},{"cell_type":"markdown","source":["Ventajas:\n","- Fácil acceso\n","- Ejecutar Spark en prácticamente cualquier dispositivo, los recursos están en la nube.\n","- Como los recursos están la nube, no hay que preocuparse por los recursos de hardware\n","- Trabajo en equipo, más sencillo el trabajo colaborativo. Varias personas pueden trabajar sobre un mismo notebook.\n","\n","\n","Desventajas:\n","- No se guardan las configuraciones de Spark luego de un tiempo\n","> No obstante el notebook permanece intacto. Se puede volver a ejecutar las líneas de código para tener la configuración nuevamente.\n","- Escalabilidad, como el servicio es gratuito, los recursos son limitados.\n","> Para llevarlo a ambientes productivos, necesitamos una infraestructura capaz de brindarnos estas especificaciones."],"metadata":{"id":"AzJLfwssmyU6"}},{"cell_type":"markdown","source":["## Instalaciones para ejecutar Spark en Colab"],"metadata":{"id":"CqjAFIHMJoQc"}},{"cell_type":"markdown","source":["### `Descarga e instalación de Apache Spark en Colab`"],"metadata":{"id":"LccNQX8WMBtl"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"JPNdekO7KHTp"},"outputs":[],"source":["# Instalar SDK Java 8\n","!apt-get install openjdk-8-jdk-headless -qq > /dev/null"]},{"cell_type":"code","source":["# Descargar Spark 3.2.4\n","!wget -q https://archive.apache.org/dist/spark/spark-3.2.4/spark-3.2.4-bin-hadoop3.2.tgz"],"metadata":{"id":"peqw7BfRKjHE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Descomprimir el archivo descargado de Spark\n","!tar xf spark-3.2.4-bin-hadoop3.2.tgz"],"metadata":{"id":"wIsOwRkJKjEi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Establecer las variables de entorno\n","import os\n","\n","os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n","os.environ[\"SPARK_HOME\"] = \"/content/spark-3.2.4-bin-hadoop3.2\""],"metadata":{"id":"sUtXLbwuKjBH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Instalar la librería findspark\n","!pip install -q findspark"],"metadata":{"id":"oMM5SJEPKi_X"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Instalar pyspark\n","!pip install -q pyspark"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"51AZCLcVKi9x","executionInfo":{"status":"ok","timestamp":1708225550840,"user_tz":180,"elapsed":36078,"user":{"displayName":"Nahuel Lopez","userId":"06859695819217714267"}},"outputId":"f9126ed1-b460-4e91-ea89-35223d84a511"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.9/316.9 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n"]}]},{"cell_type":"code","source":["### verificar la instalación ###\n","import findspark\n","findspark.init()\n","\n","from pyspark.sql import SparkSession\n","spark = SparkSession.builder.master(\"local[*]\").getOrCreate()"],"metadata":{"id":"2AMEAXrbKi8o"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Probando la sesión de Spark\n","df = spark.createDataFrame([{\"Hola\": \"Mundo\"} for x in range(10)])\n","# df.show(10, False)\n","df.show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QivCkbKRKi6g","executionInfo":{"status":"ok","timestamp":1708226374480,"user_tz":180,"elapsed":6201,"user":{"displayName":"Nahuel Lopez","userId":"06859695819217714267"}},"outputId":"09e383c5-012f-421b-bb97-d329a12167b4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["+-----+\n","| Hola|\n","+-----+\n","|Mundo|\n","|Mundo|\n","|Mundo|\n","|Mundo|\n","|Mundo|\n","|Mundo|\n","|Mundo|\n","|Mundo|\n","|Mundo|\n","|Mundo|\n","+-----+\n","\n"]}]},{"cell_type":"markdown","source":["### `Descarga e instalación de Apache Spark en una sola celda`"],"metadata":{"id":"U_xdI5-TOhbo"}},{"cell_type":"code","source":["# Instalar SDK Java 8\n","!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n","\n","# Descargar Spark 3.2.4\n","!wget -q https://archive.apache.org/dist/spark/spark-3.2.4/spark-3.2.4-bin-hadoop3.2.tgz\n","\n","# Descomprimir el archivo descargado de Spark\n","!tar xf spark-3.2.4-bin-hadoop3.2.tgz\n","\n","# Establecer las variables de entorno\n","import os\n","\n","os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n","os.environ[\"SPARK_HOME\"] = \"/content/spark-3.2.4-bin-hadoop3.2\"\n","\n","# Instalar la librería findspark\n","!pip install -q findspark\n","\n","# Instalar pyspark\n","!pip install -q pyspark\n","\n","### verificar la instalación ###\n","import findspark\n","findspark.init()\n","\n","from pyspark.sql import SparkSession\n","spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n","\n","# Probando la sesión de Spark\n","df = spark.createDataFrame([{\"Hola\": \"Mundo\"} for x in range(10)])\n","# df.show(10, False)\n","df.show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wEvprYN0Ot1l","executionInfo":{"status":"ok","timestamp":1708226674655,"user_tz":180,"elapsed":28873,"user":{"displayName":"Nahuel Lopez","userId":"06859695819217714267"}},"outputId":"a03c5e4e-42a5-415f-925a-aae5e7c348fb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["+-----+\n","| Hola|\n","+-----+\n","|Mundo|\n","|Mundo|\n","|Mundo|\n","|Mundo|\n","|Mundo|\n","|Mundo|\n","|Mundo|\n","|Mundo|\n","|Mundo|\n","|Mundo|\n","+-----+\n","\n"]}]},{"cell_type":"markdown","source":["## Spark UI en Colab"],"metadata":{"id":"q5eOVSiKAhWl"}},{"cell_type":"markdown","source":[],"metadata":{"id":"reeY5a8dkuA_"}},{"cell_type":"code","source":["# Instalar SDK java 8\n","\n","!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n","\n","# Descargar Spark\n","\n","!wget -q https://archive.apache.org/dist/spark/spark-3.3.4/spark-3.3.4-bin-hadoop3.tgz\n","\n","# Descomprimir la version de Spark\n","\n","!tar xf spark-3.3.4-bin-hadoop3.tgz\n","\n","# Establecer las variables de entorno\n","\n","import os\n","\n","os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n","os.environ[\"SPARK_HOME\"] = \"/content/spark-3.3.4-bin-hadoop3\"\n","\n","# Descargar findspark\n","\n","!pip install -q findspark\n","\n","# Crear la sesión de Spark\n","\n","import findspark\n","\n","findspark.init()\n","\n","from pyspark.sql import SparkSession\n","\n","spark = (\n","    SparkSession.builder\n","    .config('spark.ui.port', '4050')\n","    .getOrCreate()\n",")\n","\n","from google.colab import output\n","\n","output.serve_kernel_port_as_window(4050, path='/jobs/index.html')\n","\n","from pyspark.sql.functions import col\n","\n","spark.range(10000).toDF(\"id\").filter(col('id') / 2 == 0).write.mode('overwrite').parquet('/output')"],"metadata":{"id":"H1IQB2NmAleg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Spark SQL Básico"],"metadata":{"id":"4xTFXrXNSG_b"}},{"cell_type":"markdown","source":["### Dataframes: fuentes de datos I"],"metadata":{"id":"AONlSDOoxVza"}},{"cell_type":"code","source":["# Creando DataFrames\n","\n","import findspark\n","findspark.init()\n","from pyspark.sql import SparkSession\n","\n","spark = SparkSession.builder.getOrCreate()\n","sc = spark.sparkContext\n","\n","rdd = sc.parallelize([item for item in range(10)]).map(lambda x: (x, x ** 2))\n","\n","rdd.collect()\n","\n","df = rdd.toDF(['numero', 'cudrado'])\n","\n","df.printSchema()\n","\n","df.show()\n","\n","# Crear un DataFrame a partir de un RDD con schema\n","\n","rdd1 = sc.parallelize([(1, 'Jose', 35.5), (2, 'Teresa', 54.3), (3, 'Katia', 12.7)])\n","\n","from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType\n","\n","# Primera vía\n","\n","esquema1 = StructType(\n","    [\n","     StructField('id', IntegerType(), True),\n","     StructField('nombre', StringType(), True),\n","     StructField('saldo', DoubleType(), True)\n","    ]\n",")\n","\n","# Segunda vía\n","\n","esquema2 = \"`id` INT, `nombre` STRING, `saldo` DOUBLE\"\n","\n","df1 = spark.createDataFrame(rdd1, schema=esquema1)\n","\n","df1.printSchema()\n","\n","df1.show()\n","\n","df2 = spark.createDataFrame(rdd1, schema=esquema2)\n","\n","df2.printSchema()\n","\n","df2.show()\n","\n","# Crear un DataFrame a partir de un rango de números\n","\n","spark.range(5).toDF('id').show()\n","\n","spark.range(3, 15).toDF('id').show()\n","\n","spark.range(0, 20, 2).toDF('id').show()\n","\n"],"metadata":{"id":"UA7iE-ksSL38"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Dataframes: fuentes de datos II"],"metadata":{"id":"3HYKvYDfxkkF"}},{"cell_type":"code","source":["# Creando DataFrames\n","\n","import findspark\n","findspark.init()\n","from pyspark.sql import SparkSession\n","\n","spark = SparkSession.builder.getOrCreate()\n","sc = spark.sparkContext\n","\n","rdd = sc.parallelize([item for item in range(10)]).map(lambda x: (x, x ** 2))\n","\n","rdd.collect()\n","\n","df = rdd.toDF(['numero', 'cudrado'])\n","\n","df.printSchema()\n","\n","df.show()\n","\n","# Crear un DataFrame a partir de un RDD con schema\n","\n","rdd1 = sc.parallelize([(1, 'Jose', 35.5), (2, 'Teresa', 54.3), (3, 'Katia', 12.7)])\n","\n","from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType\n","\n","# Primera vía\n","\n","esquema1 = StructType(\n","    [\n","     StructField('id', IntegerType(), True),\n","     StructField('nombre', StringType(), True),\n","     StructField('saldo', DoubleType(), True)\n","    ]\n",")\n","\n","# Segunda vía\n","\n","esquema2 = \"`id` INT, `nombre` STRING, `saldo` DOUBLE\"\n","\n","df1 = spark.createDataFrame(rdd1, schema=esquema1)\n","\n","df1.printSchema()\n","\n","df1.show()\n","\n","df2 = spark.createDataFrame(rdd1, schema=esquema2)\n","\n","df2.printSchema()\n","\n","df2.show()\n","\n","# Crear un DataFrame a partir de un rango de números\n","\n","spark.range(5).toDF('id').show()\n","\n","spark.range(3, 15).toDF('id').show()\n","\n","spark.range(0, 20, 2).toDF('id').show()\n","\n"],"metadata":{"id":"vVtpG6-GSMYe"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Trabajo con columnas"],"metadata":{"id":"NtTo8TzaIfoU"}},{"cell_type":"code","source":["# Trabajo con columnas\n","\n","import findspark\n","findspark.init()\n","from pyspark.sql import SparkSession\n","\n","spark = SparkSession.builder.getOrCreate()\n","\n","df = spark.read.parquet('./data/dataPARQUET.parquet')\n","\n","df.printSchema()\n","\n","# Primera alternativa para referirnos a las columnas\n","\n","df.select('title').show()\n","\n","# Segunda alternativa\n","\n","from pyspark.sql.functions import col\n","\n","df.select(col('title')).show()\n","\n"],"metadata":{"id":"DKZHSUcKSMWp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Transformaciones, funciones, select y selectExpr"],"metadata":{"id":"cs1KbFDTL08b"}},{"cell_type":"code","source":["# Transformaciones - funciones select y selectExpr\n","\n","import findspark\n","findspark.init()\n","from pyspark.sql import SparkSession\n","\n","spark = SparkSession.builder.getOrCreate()\n","\n","# select\n","\n","df = spark.read.parquet('./data/datos.parquet')\n","\n","df.printSchema()\n","\n","from pyspark.sql.functions import col\n","\n","df.select(col('video_id')).show()\n","\n","df.select('video_id', 'trending_date').show()\n","\n","# Esta vía nos dará error\n","\n","df.select(\n","    'likes',\n","    'dislikes',\n","    ('likes' - 'dislikes')\n",").show()\n","\n","# Forma correcta\n","\n","df.select(\n","    col('likes'),\n","    col('dislikes'),\n","    (col('likes') - col('dislikes')).alias('aceptacion')\n",").show()\n","\n","# selectExpr\n","\n","df.selectExpr('likes', 'dislikes', '(likes - dislikes) as aceptacion').show()\n","\n","df.selectExpr(\"count(distinct(video_id)) as videos\").show()\n","\n"],"metadata":{"id":"1uFizpl8SMU7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Transformaciones, funciones, filter y where"],"metadata":{"id":"xiAz20BnMJta"}},{"cell_type":"code","source":["# Transformaciones - funciones filter y where\n","\n","import findspark\n","findspark.init()\n","from pyspark.sql import SparkSession\n","\n","spark = SparkSession.builder.getOrCreate()\n","\n","df = spark.read.parquet('./data/datos.parquet')\n","\n","# filter\n","\n","from pyspark.sql.functions import col\n","\n","df.show()\n","\n","df.filter(col('video_id') == '2kyS6SvSYSE').show()\n","\n","df1 = spark.read.parquet('./data/datos.parquet').where(col('trending_date') != '17.14.11')\n","\n","df1.show()\n","\n","df2 = spark.read.parquet('./data/datos.parquet').where(col('likes') > 5000)\n","\n","df2.filter((col('trending_date') != '17.14.11') & (col('likes') > 7000)).show()\n","\n","df2.filter(col('trending_date') != '17.14.11').filter(col('likes') > 7000).show()\n"],"metadata":{"id":"z6qA-SFaSMTF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Transformaciones, funciones, distinct y dropDuplicates"],"metadata":{"id":"p_oJWORyMh8U"}},{"cell_type":"code","source":["# Transformaciones - funciones distinct y dropDuplicates\n","\n","import findspark\n","findspark.init()\n","from pyspark.sql import SparkSession\n","\n","spark = SparkSession.builder.getOrCreate()\n","\n","df = spark.read.parquet('./data')\n","\n","# distinct\n","\n","df_sin_duplicados = df.distinct()\n","\n","print('El conteo del dataframe original es {}'.format(df.count()))\n","print('El conteo del dataframe sin duplicados es {}'.format(df_sin_duplicados.count()))\n","\n","# función dropDuplicates\n","\n","dataframe = spark.createDataFrame([(1, 'azul', 567), (2, 'rojo', 487), (1, 'azul', 345), (2, 'verde', 783)]).toDF('id', 'color', 'importe')\n","\n","dataframe.show()\n","\n","dataframe.dropDuplicates(['id', 'color']).show()"],"metadata":{"id":"8NiX0Qk2Mih5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Transformaciones, funciones, sort y orderBy"],"metadata":{"id":"2LjNzIixMlyz"}},{"cell_type":"code","source":["# Transformaciones - funciones sort y orderBy\n","\n","import findspark\n","findspark.init()\n","from pyspark.sql import SparkSession\n","\n","spark = SparkSession.builder.getOrCreate()\n","\n","from pyspark.sql.functions import col\n","\n","df = (spark.read.parquet('./data')\n","    .select(col('likes'), col('views'), col('video_id'), col('dislikes'))\n","    .dropDuplicates(['video_id'])\n",")\n","\n","df.show()\n","\n","# sort\n","\n","df.sort('likes').show()\n","\n","from pyspark.sql.functions import desc\n","\n","df.sort(desc('likes')).show()\n","\n","# función orderBy\n","\n","df.orderBy(col('views')).show()\n","\n","df.orderBy(col('views').desc()).show()\n","\n","dataframe = spark.createDataFrame([(1, 'azul', 568), (2, 'rojo', 235), (1, 'azul', 456), (2, 'azul', 783)]).toDF('id', 'color', 'importe')\n","\n","dataframe.show()\n","\n","dataframe.orderBy(col('color').desc(), col('importe')).show()\n","\n","# funcion limit\n","\n","top_10 = df.orderBy(col('views').desc()).limit(10)\n","\n","top_10.show()\n"],"metadata":{"id":"QuotBztUMmPH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Transformaciones, funciones, withColumn y withColumnRenamed"],"metadata":{"id":"SGlX53FJMmpp"}},{"cell_type":"code","source":["# Transformaciones - funciones withColumn y withColumnRenamed\n","\n","import findspark\n","findspark.init()\n","from pyspark.sql import SparkSession\n","\n","spark = SparkSession.builder.getOrCreate()\n","\n","df = spark.read.parquet('./data')\n","\n","# withColumn\n","\n","from pyspark.sql.functions import col\n","\n","df_valoracion = df.withColumn('valoracion', col('likes') - col('dislikes'))\n","\n","df_valoracion.printSchema()\n","\n","df_valoracion1 = (df.withColumn('valoracion', col('likes') - col('dislikes'))\n","                    .withColumn('res_div', col('valoracion') % 10)\n",")\n","\n","df_valoracion1.printSchema()\n","\n","df_valoracion1.select(col('likes'), col('dislikes'), col('valoracion'), col('res_div')).show()\n","\n","# withColumnRenamed\n","\n","df_renombrado = df.withColumnRenamed('video_id', 'id')\n","\n","df_renombrado.printSchema()\n","\n","df_error = df.withColumnRenamed('nombre_que_no_existe', 'otro_nombre')\n","\n","df_error.printSchema()\n"],"metadata":{"id":"O9xWLWXBMn_-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Transformaciones, funciones, drop, sample, randomSplit"],"metadata":{"id":"3xzrZ4O-ModN"}},{"cell_type":"code","source":["# Transformaciones - funciones drop, sample y randomSplit\n","\n","import findspark\n","findspark.init()\n","from pyspark.sql import SparkSession\n","\n","spark = SparkSession.builder.getOrCreate()\n","\n","df = spark.read.parquet('./data')\n","\n","# drop\n","\n","df.printSchema()\n","\n","df_util = df.drop('comments_disabled')\n","\n","df_util.printSchema()\n","\n","df_util = df.drop('comments_disabled', 'ratings_disabled', 'thumbnail_link')\n","\n","df_util.printSchema()\n","\n","df_util = df.drop('comments_disabled', 'ratings_disabled', 'thumbnail_link', 'cafe')\n","\n","df_util.printSchema()\n","\n","# sample\n","\n","df_muestra = df.sample(0.8)\n","\n","num_filas = df.count()\n","num_filas_muestra = df_muestra.count()\n","\n","print('El 80% de filas del dataframe original es {}'.format(num_filas - (num_filas*0.2)))\n","print('El numero de filas del dataframe muestra es {}'.format(num_filas_muestra))\n","\n","df_muestra = df.sample(fraction=0.8, seed=1234)\n","\n","df_muestra = df.sample(withReplacement=True, fraction=0.8, seed=1234)\n","\n","# randomSplit\n","\n","train, test = df.randomSplit([0.8, 0.2], seed=1234)\n","\n","train, validation, test = df.randomSplit([0.6, 0.2, 0.2], seed=1234)\n","\n","train.count()\n","\n","validation.count()\n","\n","test.count()\n"],"metadata":{"id":"lBlIb0SxMo7F"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Trabajo con datos incorrectos faltantes"],"metadata":{"id":"rbFVOU_hMpeL"}},{"cell_type":"code","source":["# Trabajo con datos incorrectos o faltantes\n","\n","import findspark\n","findspark.init()\n","from pyspark.sql import SparkSession\n","\n","spark = SparkSession.builder.getOrCreate()\n","\n","df = spark.read.parquet('./data/')\n","\n","df.count()\n","\n","df.na.drop().count()\n","\n","df.na.drop('any').count()\n","\n","df.dropna().count()\n","\n","df.na.drop(subset=['views']).count()\n","\n","df.na.drop(subset=['views', 'dislikes']).count()\n","\n","from pyspark.sql.functions import col\n","\n","df.orderBy(col('views')).select(col('views'), col('likes'), col('dislikes')).show()\n","\n","df.fillna(0).orderBy(col('views')).select(col('views'), col('likes'), col('dislikes')).show()\n","\n","df.fillna(0, subset=['likes', 'dislikes']).orderBy(col('views')).select(col('views'), col('likes'), col('dislikes')).show()\n"],"metadata":{"id":"I19kHepLMp-v"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Acciones sobre un dataframe"],"metadata":{"id":"7p6_VdbHMsQi"}},{"cell_type":"code","source":["# Acciones sobre un dataframe en Spark SQL\n","\n","import findspark\n","findspark.init()\n","from pyspark.sql import SparkSession\n","\n","spark = SparkSession.builder.getOrCreate()\n","\n","df = spark.read.parquet('./data/')\n","\n","# show\n","\n","df.show()\n","\n","df.show(5)\n","\n","df.show(5, truncate=False)\n","\n","# take\n","\n","df.take(1)\n","\n","# head\n","\n","df.head(1)\n","\n","# collect\n","\n","df.select('likes').collect()"],"metadata":{"id":"5tl1aFT8Mstx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Escritura de dataframes"],"metadata":{"id":"pKKo3Kt2MtSN"}},{"cell_type":"code","source":["# Escritura de DataFrames\n","\n","import findspark\n","findspark.init()\n","from pyspark.sql import SparkSession\n","\n","spark = SparkSession.builder.getOrCreate()\n","\n","df = spark.read.parquet('./data/')\n","\n","df1 = df.repartition(2)\n","\n","df1.write.format('csv').option('sep', '|').save()\n","\n","df1.coalesce(1).write.format('csv').option('sep', '|').save('./output/csv1')\n","\n","df.printSchema()\n","\n","df.select('comments_disabled').distinct().show()\n","\n","from pyspark.sql.functions import col\n","\n","df_limpio = df.filter(col('comments_disabled').isin('True', 'False'))\n","\n","df_limpio.write.partitionBy('comments_disabled').parquet('./output/parquet')\n"],"metadata":{"id":"8_bIFXbnMtxY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Lectura 1"],"metadata":{"id":"f4N3NCn5MuQx"}},{"cell_type":"code","source":["# Instalar SDK java 8\n","\n","!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n","\n","# Descargar Spark\n","\n","!wget -q https://archive.apache.org/dist/spark/spark-3.3.4/spark-3.3.4-bin-hadoop3.tgz\n","\n","# Descomprimir la version de Spark\n","\n","!tar xf spark-3.3.4-bin-hadoop3.tgz\n","\n","# Establecer las variables de entorno\n","\n","import os\n","\n","os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n","os.environ[\"SPARK_HOME\"] = \"/content/spark-3.3.4-bin-hadoop3\"\n","\n","# Descargar findspark\n","\n","!pip install -q findspark\n","\n","# Instalar dotenv para manejar las credenciales\n","\n","!pip install python-dotenv\n","\n","# Extraer las credenciales del archivo .env a un diccionario de Python\n","\n","from dotenv import dotenv_values\n","\n","config = dotenv_values(\".env\")\n","\n","# Crear la sesión de Spark con las configuraciones necesarias para conectarse a AWS S3\n","\n","import findspark\n","\n","findspark.init()\n","\n","from pyspark.sql import SparkSession\n","\n","spark = (SparkSession\n","         .builder\n","         .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:3.3.1,com.amazonaws:aws-java-sdk-bundle:1.11.469\")\n","         .config(\"spark.hadoop.fs.s3a.aws.credentials.provider\", \"org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider\")\n","         .getOrCreate()\n","         )\n","\n","# Extraer las credenciales del diccionario\n","\n","accessKeyId=config.get('ACCESS_KEY')\n","secretAccessKey=config.get('SECRET_ACCESS_KEY')\n","\n","# Establecer las configuraciones de Hodoop necesarias\n","\n","sc = spark.sparkContext\n","\n","sc._jsc.hadoopConfiguration().set('fs.s3a.access.key', accessKeyId)\n","sc._jsc.hadoopConfiguration().set('fs.s3a.secret.key', secretAccessKey)\n","sc._jsc.hadoopConfiguration().set('fs.s3a.path.style.access', 'true')\n","sc._jsc.hadoopConfiguration().set('fs.s3a.impl', 'org.apache.hadoop.fs.s3a.S3AFileSystem')\n","sc._jsc.hadoopConfiguration().set('fs.s3a.endpoint', 's3.amazonaws.com')\n","\n","df = spark.read.parquet('s3a://josemtech/parquet')\n","\n","df.show()\n","\n","df1 = spark.read.option('header', 'true').option('inferSchema', 'true').csv('s3a://josemtech/csv/')\n","\n","df1.show()\n","\n","df.write.mode('overwrite').parquet('s3a://josemtech/salida')"],"metadata":{"id":"-8T8EdA4Muu1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Lectura 2"],"metadata":{"id":"MvNwF1XnMvIV"}},{"cell_type":"code","source":["# Instalar SDK java 8\n","\n","!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n","\n","# Descargar Spark\n","\n","!wget -q https://archive.apache.org/dist/spark/spark-3.3.4/spark-3.3.4-bin-hadoop3.tgz\n","\n","# Descomprimir la version de Spark\n","\n","!tar xf spark-3.3.4-bin-hadoop3.tgz\n","\n","# Establecer las variables de entorno\n","\n","import os\n","\n","os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n","os.environ[\"SPARK_HOME\"] = \"/content/spark-3.3.4-bin-hadoop3\"\n","\n","# Descargar findspark\n","\n","!pip install -q findspark\n","\n","# Extraer las credenciales desde los Secrets\n","\n","from google.colab import userdata\n","\n","account_key = userdata.get('ACCOUNT_KEY')\n","\n","# Crear la sesión de Spark\n","\n","import findspark\n","\n","findspark.init()\n","\n","from pyspark.sql import SparkSession\n","\n","spark = (SparkSession.builder\n","         .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-azure:3.3.6,com.microsoft.azure:azure-storage:8.6.6\")\n","         .config(\"spark.hadoop.fs.azure.account.key.josemtech.blob.core.windows.net\", account_key)\n","         .config(\"spark.hadoop.fs.wasbs.impl\", \"org.apache.hadoop.fs.azure.NativeAzureFileSystem\")\n","         .config(\"spark.hadoop.fs.azure\", \"org.apache.hadoop.fs.azure.NativeAzureFileSystem\")\n","         .getOrCreate())\n","\n","# Luctura\n","\n","df = spark.read.parquet(\"wasbs://spark-data@josemtech.blob.core.windows.net/parquet\")\n","\n","df.show()\n","\n","# Escritura\n","\n","df.write.mode(\"overwrite\").parquet(\"wasbs://spark-data@josemtech.blob.core.windows.net/test/\")"],"metadata":{"id":"T89b3e_bMvnO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Lectura 3"],"metadata":{"id":"wButYQ9uMxQE"}},{"cell_type":"code","source":["# Instalar SDK java 8\n","\n","!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n","\n","# Descargar Spark\n","\n","!wget -q https://archive.apache.org/dist/spark/spark-3.3.4/spark-3.3.4-bin-hadoop3.tgz\n","\n","# Descomprimir la version de Spark\n","\n","!tar xf spark-3.3.4-bin-hadoop3.tgz\n","\n","# Establecer las variables de entorno\n","\n","import os\n","\n","os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n","os.environ[\"SPARK_HOME\"] = \"/content/spark-3.3.4-bin-hadoop3\"\n","\n","# Descargar findspark\n","\n","!pip install -q findspark\n","\n","# Descargar el jar necesario para conectarse al bucket de GCP\n","\n","!wget https://repo1.maven.org/maven2/com/google/cloud/bigdataoss/gcs-connector/hadoop3-2.2.9/gcs-connector-hadoop3-2.2.9-shaded.jar\n","\n","# Mover el jar descargado a la carpeta de jars de Spark\n","\n","!mv gcs-connector-hadoop3-2.2.9-shaded.jar /content/spark-3.4.2-bin-hadoop3/jars\n","\n","# Crear la sesión de Spark\n","\n","import findspark\n","\n","findspark.init()\n","\n","from pyspark.sql import SparkSession\n","\n","spark = (SparkSession.builder\n","         .config(\"spark.hadoop.fs.gs.impl\",\"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\")\n","         .config(\"google.cloud.auth.service.account.json.keyfile\",\"/content/pyspark.json\")\n","         .getOrCreate())\n","\n","df = spark.read.parquet('gs://josemtech/parquet')\n","\n","df.show()\n","\n","df.write.mode('overwrite').parquet('gs://josemtech/salida_parquet')\n","\n","df1 = spark.read.option('header', 'true').csv('gs://josemtech/csv')\n","\n","df1.show()\n","\n","df1.write.mode('overwrite').csv('gs://josemtech/salida_csv')"],"metadata":{"id":"iyIJU9lWMxqp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Persistencia de dataframes"],"metadata":{"id":"UlhE480VMycw"}},{"cell_type":"code","source":["# Persistencia de DataFrames\n","\n","import findspark\n","findspark.init()\n","from pyspark.sql import SparkSession\n","\n","spark = SparkSession.builder.getOrCreate()\n","\n","df = spark.createDataFrame([(1, 'a'), (2, 'b'), (3, 'c')], ['id', 'valor'])\n","\n","df.show()\n","\n","df.persist()\n","\n","df.unpersist()\n","\n","df.cache()\n","\n","from pyspark.storagelevel import StorageLevel\n","\n","df.persist(StorageLevel.DISK_ONLY)\n","\n","df.persist(StorageLevel.MEMORY_AND_DISK)\n"],"metadata":{"id":"mu6fact2My6s"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Ejercicios"],"metadata":{"id":"P1wVBHtsMzTy"}},{"cell_type":"code","source":[],"metadata":{"id":"nfs86Gc5Mz8b"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Spark SQL Avanzado"],"metadata":{"id":"vw6VZXSpSNZA"}},{"cell_type":"markdown","source":[],"metadata":{"id":"v-G812zxe7SN"}},{"cell_type":"markdown","source":["### Agregaciones"],"metadata":{"id":"KkpLftPxe3bN"}},{"cell_type":"code","source":["# Explorando los datos\n","\n","import findspark\n","findspark.init()\n","from pyspark.sql import SparkSession\n","\n","spark = SparkSession.builder.getOrCreate()\n","\n","df = spark.read.parquet('./data/')\n","\n","df.printSchema()\n","\n","df.show(20, truncate=False)\n"],"metadata":{"id":"J8Pg82cqSUci"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Funciones"],"metadata":{"id":"0zJIVCCwe8Ba"}},{"cell_type":"code","source":["# Funciones count, countDistinct y approx_count_distinct\n","\n","import findspark\n","findspark.init()\n","from pyspark.sql import SparkSession\n","\n","spark = SparkSession.builder.getOrCreate()\n","\n","df = spark.read.parquet('./data/dataframe')\n","\n","df.printSchema()\n","\n","df.show()\n","\n","# count\n","\n","from pyspark.sql.functions import count\n","\n","df.select(\n","    count('nombre').alias('conteo_nombre'),\n","    count('color').alias('conteo_color')\n",").show()\n","\n","df.select(\n","    count('nombre').alias('conteo_nombre'),\n","    count('color').alias('conteo_color'),\n","    count('*').alias('conteo_general')\n",").show()\n","\n","# countDistinct\n","\n","from pyspark.sql.functions import countDistinct\n","\n","df.select(\n","    countDistinct('color').alias('colores_dif')\n",").show()\n","\n","# approx_count_distinct\n","\n","from pyspark.sql.functions import approx_count_distinct\n","\n","dataframe = spark.read.parquet('./data/vuelos')\n","\n","dataframe.printSchema()\n","\n","dataframe.select(\n","    countDistinct('AIRLINE'),\n","    approx_count_distinct('AIRLINE')\n",").show()\n"],"metadata":{"id":"T3Z9A3yzSVUt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Funciones min y max"],"metadata":{"id":"g1z5xso0e90P"}},{"cell_type":"code","source":["# Funciones min y max\n","\n","import findspark\n","findspark.init()\n","from pyspark.sql import SparkSession\n","\n","spark = SparkSession.builder.getOrCreate()\n","\n","vuelos = spark.read.parquet('./data')\n","\n","vuelos.printSchema()\n","\n","from pyspark.sql.functions import min, max, col\n","\n","vuelos.select(\n","    min('AIR_TIME').alias('menor_timepo'),\n","    max('AIR_TIME').alias('mayor_tiempo')\n",").show()\n","\n","vuelos.select(\n","    min('AIRLINE_DELAY'),\n","    max('AIRLINE_DELAY')\n",").show()\n"],"metadata":{"id":"s0B2ZOlCSVTB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Funciones sum y sumDistinct"],"metadata":{"id":"p4Qqkqe5fBhq"}},{"cell_type":"code","source":["# Funciones sum, sumDistinct y avg\n","\n","import findspark\n","findspark.init()\n","from pyspark.sql import SparkSession\n","\n","spark = SparkSession.builder.getOrCreate()\n","\n","vuelos = spark.read.parquet('./data/')\n","\n","from pyspark.sql.functions import sum, sumDistinct, avg, count\n","\n","# sum\n","\n","vuelos.printSchema()\n","\n","vuelos.select(\n","    sum('DISTANCE').alias('sum_dis')\n",").show()\n","\n","# sumDistinct\n","\n","vuelos.select(\n","    sumDistinct('DISTANCE').alias('sum_dis_dif')\n",").show()\n","\n","# avg\n","\n","vuelos.select(\n","    avg('AIR_TIME').alias('promedio_aire'),\n","    (sum('AIR_TIME') / count('AIR_TIME')).alias('prom_manual')\n",").show()\n"],"metadata":{"id":"v1xQbT4kSVRG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Agregación con agrupación"],"metadata":{"id":"sEqgOplGf6rT"}},{"cell_type":"code","source":["# Agregación con agrupación\n","\n","import findspark\n","findspark.init()\n","from pyspark.sql import SparkSession\n","\n","spark = SparkSession.builder.getOrCreate()\n","\n","vuelos = spark.read.parquet('./data/')\n","\n","vuelos.printSchema()\n","\n","from pyspark.sql.functions import desc\n","\n","(vuelos.groupBy('ORIGIN_AIRPORT')\n","    .count()\n","    .orderBy(desc('count'))\n",").show()\n","\n","(vuelos.groupBy('ORIGIN_AIRPORT', 'DESTINATION_AIRPORT')\n","    .count()\n","    .orderBy(desc('count'))\n",").show()\n"],"metadata":{"id":"ToBOY8kQTIQt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Varias agregaciones por grupo"],"metadata":{"id":"CWDte3lOf-KC"}},{"cell_type":"code","source":["# Varias agregaciones por grupo\n","\n","import findspark\n","findspark.init()\n","from pyspark.sql import SparkSession\n","\n","spark = SparkSession.builder.getOrCreate()\n","\n","vuelos = spark.read.parquet('./data/')\n","\n","from pyspark.sql.functions import count, min, max, desc, avg\n","\n","vuelos.groupBy('ORIGIN_AIRPORT').agg(\n","    count('AIR_TIME').alias('tiempo_aire'),\n","    min('AIR_TIME').alias('min'),\n","    max('AIR_TIME').alias('max')\n",").orderBy(desc('tiempo_aire')).show()\n","\n","vuelos.groupBy('MONTH').agg(\n","    count('ARRIVAL_DELAY').alias('conteo_de_retrasos'),\n","    avg('DISTANCE').alias('prom_dist')\n",").orderBy(desc('conteo_de_retrasos')).show()\n"],"metadata":{"id":"bSSxwk-hTIOY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Agregación con pivote"],"metadata":{"id":"YiptC7kTgCM6"}},{"cell_type":"code","source":["# Agregación con pivote\n","\n","import findspark\n","findspark.init()\n","from pyspark.sql import SparkSession\n","\n","spark = SparkSession.builder.getOrCreate()\n","\n","estudiantes = spark.read.parquet('./data/')\n","\n","estudiantes.show()\n","\n","from pyspark.sql.functions import min, max, avg, col\n","\n","estudiantes.groupBy('graduacion').pivot('sexo').agg(avg('peso')).show()\n","\n","estudiantes.groupBy('graduacion').pivot('sexo').agg(avg('peso'), min('peso'), max('peso')).show()\n","\n","estudiantes.groupBy('graduacion').pivot('sexo', ['M']).agg(avg('peso'), min('peso'), max('peso')).show()\n","\n","estudiantes.groupBy('graduacion').pivot('sexo', ['F']).agg(avg('peso'), min('peso'), max('peso')).show()\n"],"metadata":{"id":"6YZx8tb1TIME"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Inner Join"],"metadata":{"id":"cLpY1rmEgPet"}},{"cell_type":"code","source":["# Inner Join\n","\n","import findspark\n","findspark.init()\n","from pyspark.sql import SparkSession\n","\n","spark = SparkSession.builder.getOrCreate()\n","\n","empleados = spark.read.parquet('./data/empleados')\n","\n","departamentos = spark.read.parquet('./data/departamentos')\n","\n","empleados.show()\n","\n","departamentos.show()\n","\n","# Inner join\n","\n","from pyspark.sql.functions import col\n","\n","join_df = empleados.join(departamentos, col('num_dpto') == col('id'))\n","\n","join_df.show()\n","\n","join_df = empleados.join(departamentos, col('num_dpto') == col('id'), 'inner')\n","\n","join_df.show()\n","\n","join_df = empleados.join(departamentos).where(col('num_dpto') == col('id'))\n","\n","join_df.show()\n"],"metadata":{"id":"_waX_gIIgP3q"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Left Outer Join"],"metadata":{"id":"7KfvGnhbgQZy"}},{"cell_type":"code","source":["# Left Outer Join\n","\n","import findspark\n","findspark.init()\n","from pyspark.sql import SparkSession\n","\n","spark = SparkSession.builder.getOrCreate()\n","\n","empleados = spark.read.parquet('./data/empleados/')\n","\n","departamentos = spark.read.parquet('./data/departamentos/')\n","\n","from pyspark.sql.functions import col\n","\n","empleados.join(departamentos, col('num_dpto') == col('id'), 'leftouter').show()\n","\n","empleados.join(departamentos, col('num_dpto') == col('id'), 'left_outer').show()\n","\n","empleados.join(departamentos, col('num_dpto') == col('id'), 'left').show()\n"],"metadata":{"id":"fT6syaongQuo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Right Outer Join"],"metadata":{"id":"yw4AEEhRgRJ8"}},{"cell_type":"code","source":["# Right Outer Join\n","\n","import findspark\n","findspark.init()\n","from pyspark.sql import SparkSession\n","\n","spark = SparkSession.builder.getOrCreate()\n","\n","empleados = spark.read.parquet('./data/empleados/')\n","\n","departamentos = spark.read.parquet('./data/departamentos/')\n","\n","from pyspark.sql.functions import col\n","\n","empleados.join(departamentos, col('num_dpto') == col('id'), 'rightouter').show()\n","\n","empleados.join(departamentos, col('num_dpto') == col('id'), 'right_outer').show()\n","\n","empleados.join(departamentos, col('num_dpto') == col('id'), 'right').show()\n"],"metadata":{"id":"MPV0x9RKgRie"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Full Outer Join"],"metadata":{"id":"_9ZcZL6XgfSK"}},{"cell_type":"code","source":["# Full Outer Join\n","\n","import findspark\n","findspark.init()\n","from pyspark.sql import SparkSession\n","\n","spark = SparkSession.builder.getOrCreate()\n","\n","empleados = spark.read.parquet('./data/empleados/')\n","\n","departamentos = spark.read.parquet('./data/departamentos/')\n","\n","from pyspark.sql.functions import col\n","\n","empleados.join(departamentos, col('num_dpto') == col('id'), 'outer').show()\n"],"metadata":{"id":"S20OuGOfgfm9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Left Anti Join"],"metadata":{"id":"8J-s9XH4ggHo"}},{"cell_type":"code","source":["# Left Anti Join\n","\n","import findspark\n","findspark.init()\n","from pyspark.sql import SparkSession\n","\n","spark = SparkSession.builder.getOrCreate()\n","\n","empleados = spark.read.parquet('./data/empleados/')\n","\n","departamentos = spark.read.parquet('./data/departamentos/')\n","\n","from pyspark.sql.functions import col\n","\n","empleados.join(departamentos, col('num_dpto') == col('id'), 'left_anti').show()\n","\n","departamentos.join(empleados, col('num_dpto') == col('id'), 'left_anti').show()\n"],"metadata":{"id":"mgjIJmo7gglZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Left Semi Join"],"metadata":{"id":"sEmAsVQvgiLr"}},{"cell_type":"code","source":["# Left Semi Join\n","\n","import findspark\n","findspark.init()\n","from pyspark.sql import SparkSession\n","\n","spark = SparkSession.builder.getOrCreate()\n","\n","empleados = spark.read.parquet('./data/empleados/')\n","\n","departamentos = spark.read.parquet('./data/departamentos/')\n","\n","from pyspark.sql.functions import col\n","\n","empleados.join(departamentos, col('num_dpto') == col('id'), 'left_semi').show()\n"],"metadata":{"id":"UcQ0Xsj1gjCc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Cross Join"],"metadata":{"id":"vvfgFSC9gjao"}},{"cell_type":"code","source":["# Cross Join\n","\n","import findspark\n","findspark.init()\n","from pyspark.sql import SparkSession\n","\n","spark = SparkSession.builder.getOrCreate()\n","\n","empleados = spark.read.parquet('./data/empleados/')\n","\n","departamentos = spark.read.parquet('./data/departamentos/')\n","\n","from pyspark.sql.functions import col\n","\n","df = empleados.crossJoin(departamentos)\n","\n","df.show()\n","\n","df.count()"],"metadata":{"id":"di9GHBeFgjzD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Manejo de nombres de columnas duplicados"],"metadata":{"id":"DBCbKPrZgkJh"}},{"cell_type":"code","source":["# Manejo de nombres de columnas duplicados\n","\n","import findspark\n","findspark.init()\n","from pyspark.sql import SparkSession\n","\n","spark = SparkSession.builder.getOrCreate()\n","\n","empleados = spark.read.parquet('./data/empleados/')\n","\n","departamentos = spark.read.parquet('./data/departamentos/')\n","\n","from pyspark.sql.functions import col\n","\n","depa = departamentos.withColumn('num_dpto', col('id'))\n","\n","depa.printSchema()\n","\n","empleados.printSchema()\n","\n","# Devuelve un error\n","\n","empleados.join(depa, col('num_dpto') == col('num_dpto'))\n","\n","# Forma correcta\n","\n","df_con_duplicados = empleados.join(depa, empleados['num_dpto'] == depa['num_dpto'])\n","\n","df_con_duplicados.printSchema()\n","\n","df_con_duplicados.select(empleados['num_dpto']).show()\n","\n","df2 = empleados.join(depa, 'num_dpto')\n","\n","df2.printSchema()\n","\n","empleados.join(depa, ['num_dpto']).printSchema()\n"],"metadata":{"id":"FncyxGK5gk32"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Shuffle, hash join y broadcast hash join"],"metadata":{"id":"7-9rwntjglSm"}},{"cell_type":"code","source":["# Shuffle Hash Join y Broadcast Hash Join\n","\n","import findspark\n","findspark.init()\n","from pyspark.sql import SparkSession\n","\n","spark = SparkSession.builder.getOrCreate()\n","\n","empleados = spark.read.parquet('./data/empleados/')\n","\n","departamentos = spark.read.parquet('./data/departamentos/')\n","\n","from pyspark.sql.functions import col, broadcast\n","\n","empleados.join(broadcast(departamentos), col('num_dpto') == col('id')).show()\n","\n","empleados.join(broadcast(departamentos), col('num_dpto') == col('id')).explain()"],"metadata":{"id":"MM0cQYctglpe"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Ejercicios"],"metadata":{"id":"oFfGOHPVgmP_"}},{"cell_type":"code","source":[],"metadata":{"id":"iqcG8TJcgmlh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Funciones en Spark SQL"],"metadata":{"id":"NYsFpayISwks"}},{"cell_type":"markdown","source":[],"metadata":{"id":"hQWmggGTj9cD"}},{"cell_type":"markdown","source":["### Funciones de fecha y hora"],"metadata":{"id":"Me9ESk0Pj-vT"}},{"cell_type":"code","source":[],"metadata":{"id":"8nRvqo0oSx23"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Funciones de trabajo con Strings"],"metadata":{"id":"b-_XCfLdj-OR"}},{"cell_type":"code","source":[],"metadata":{"id":"ZsF10m6aSytz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Funciones de trabajo con colecciones"],"metadata":{"id":"HW-lQGZnkLLX"}},{"cell_type":"code","source":[],"metadata":{"id":"JZHKnPIESyqw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Funciones when, coalesce y lit"],"metadata":{"id":"PMFx1J3DkOr3"}},{"cell_type":"code","source":[],"metadata":{"id":"GgL2eHkuTGjR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Funciones definidas por el usuario UDF"],"metadata":{"id":"XXN17SfvkX3i"}},{"cell_type":"code","source":[],"metadata":{"id":"k8uBs6wtTGgb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Funciones de ventana"],"metadata":{"id":"Stw0QEHRkdJ_"}},{"cell_type":"code","source":[],"metadata":{"id":"CxtJs-cSTGdy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Catalyst Optimizer"],"metadata":{"id":"f9PUeF-mkf9G"}},{"cell_type":"code","source":[],"metadata":{"id":"OvwuWD4ASyok"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Ejercicios"],"metadata":{"id":"vTojhBfqkktY"}},{"cell_type":"code","source":[],"metadata":{"id":"Jl5LPyxtklJz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["-----------\n","## Fin Notebook\n","-----------"],"metadata":{"id":"d93Xay9kA95I"}}]}