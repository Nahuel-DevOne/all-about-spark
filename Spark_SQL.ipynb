{"cells":[{"cell_type":"markdown","metadata":{"id":"IhqAQ_OOKTXl"},"source":["# **Spark SQL**"]},{"cell_type":"markdown","metadata":{"id":"_zJeZZqpJu73"},"source":["## Introducción"]},{"cell_type":"markdown","metadata":{"id":"Ojkkv5J5rwg-"},"source":["### `Ventajas y desventajas de trabajar con Spark en Google Colab`"]},{"cell_type":"markdown","metadata":{"id":"AzJLfwssmyU6"},"source":["Ventajas:\n","- Fácil acceso\n","- Ejecutar Spark en prácticamente cualquier dispositivo, los recursos están en la nube.\n","- Como los recursos están la nube, no hay que preocuparse por los recursos de hardware\n","- Trabajo en equipo, más sencillo el trabajo colaborativo. Varias personas pueden trabajar sobre un mismo notebook.\n","\n","\n","Desventajas:\n","- No se guardan las configuraciones de Spark luego de un tiempo\n","> No obstante el notebook permanece intacto. Se puede volver a ejecutar las líneas de código para tener la configuración nuevamente.\n","- Escalabilidad, como el servicio es gratuito, los recursos son limitados.\n","> Para llevarlo a ambientes productivos, necesitamos una infraestructura capaz de brindarnos estas especificaciones."]},{"cell_type":"markdown","metadata":{"id":"CqjAFIHMJoQc"},"source":["## Instalaciones para ejecutar Spark en Colab"]},{"cell_type":"markdown","metadata":{"id":"LccNQX8WMBtl"},"source":["### `Descarga e instalación de Apache Spark en Colab`"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JPNdekO7KHTp"},"outputs":[],"source":["# Instalar SDK Java 8\n","!apt-get install openjdk-8-jdk-headless -qq > /dev/null"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"peqw7BfRKjHE"},"outputs":[],"source":["# Descargar Spark 3.2.4\n","!wget -q https://archive.apache.org/dist/spark/spark-3.2.4/spark-3.2.4-bin-hadoop3.2.tgz"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wIsOwRkJKjEi"},"outputs":[],"source":["# Descomprimir el archivo descargado de Spark\n","!tar xf spark-3.2.4-bin-hadoop3.2.tgz"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sUtXLbwuKjBH"},"outputs":[],"source":["# Establecer las variables de entorno\n","import os\n","\n","os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n","os.environ[\"SPARK_HOME\"] = \"/content/spark-3.2.4-bin-hadoop3.2\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oMM5SJEPKi_X"},"outputs":[],"source":["# Instalar la librería findspark\n","!pip install -q findspark"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":36078,"status":"ok","timestamp":1708225550840,"user":{"displayName":"Nahuel Lopez","userId":"06859695819217714267"},"user_tz":180},"id":"51AZCLcVKi9x","outputId":"f9126ed1-b460-4e91-ea89-35223d84a511"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.9/316.9 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n"]}],"source":["# Instalar pyspark\n","!pip install -q pyspark"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2AMEAXrbKi8o"},"outputs":[],"source":["### verificar la instalación ###\n","import findspark\n","findspark.init()\n","\n","from pyspark.sql import SparkSession\n","spark = SparkSession.builder.master(\"local[*]\").getOrCreate()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6201,"status":"ok","timestamp":1708226374480,"user":{"displayName":"Nahuel Lopez","userId":"06859695819217714267"},"user_tz":180},"id":"QivCkbKRKi6g","outputId":"09e383c5-012f-421b-bb97-d329a12167b4"},"outputs":[{"name":"stdout","output_type":"stream","text":["+-----+\n","| Hola|\n","+-----+\n","|Mundo|\n","|Mundo|\n","|Mundo|\n","|Mundo|\n","|Mundo|\n","|Mundo|\n","|Mundo|\n","|Mundo|\n","|Mundo|\n","|Mundo|\n","+-----+\n","\n"]}],"source":["# Probando la sesión de Spark\n","df = spark.createDataFrame([{\"Hola\": \"Mundo\"} for x in range(10)])\n","# df.show(10, False)\n","df.show()"]},{"cell_type":"markdown","metadata":{"id":"U_xdI5-TOhbo"},"source":["### `Descarga e instalación de Apache Spark en una sola celda`"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":28873,"status":"ok","timestamp":1708226674655,"user":{"displayName":"Nahuel Lopez","userId":"06859695819217714267"},"user_tz":180},"id":"wEvprYN0Ot1l","outputId":"a03c5e4e-42a5-415f-925a-aae5e7c348fb"},"outputs":[{"name":"stdout","output_type":"stream","text":["+-----+\n","| Hola|\n","+-----+\n","|Mundo|\n","|Mundo|\n","|Mundo|\n","|Mundo|\n","|Mundo|\n","|Mundo|\n","|Mundo|\n","|Mundo|\n","|Mundo|\n","|Mundo|\n","+-----+\n","\n"]}],"source":["# Instalar SDK Java 8\n","!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n","\n","# Descargar Spark 3.2.4\n","!wget -q https://archive.apache.org/dist/spark/spark-3.2.4/spark-3.2.4-bin-hadoop3.2.tgz\n","\n","# Descomprimir el archivo descargado de Spark\n","!tar xf spark-3.2.4-bin-hadoop3.2.tgz\n","\n","# Establecer las variables de entorno\n","import os\n","\n","os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n","os.environ[\"SPARK_HOME\"] = \"/content/spark-3.2.4-bin-hadoop3.2\"\n","\n","# Instalar la librería findspark\n","!pip install -q findspark\n","\n","# Instalar pyspark\n","!pip install -q pyspark\n","\n","### verificar la instalación ###\n","import findspark\n","findspark.init()\n","\n","from pyspark.sql import SparkSession\n","spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n","\n","# Probando la sesión de Spark\n","df = spark.createDataFrame([{\"Hola\": \"Mundo\"} for x in range(10)])\n","# df.show(10, False)\n","df.show()"]},{"cell_type":"markdown","metadata":{"id":"q5eOVSiKAhWl"},"source":["## Spark UI en Colab"]},{"cell_type":"markdown","metadata":{"id":"reeY5a8dkuA_"},"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"H1IQB2NmAleg"},"outputs":[],"source":["# Instalar SDK java 8\n","\n","!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n","\n","# Descargar Spark\n","\n","!wget -q https://archive.apache.org/dist/spark/spark-3.3.4/spark-3.3.4-bin-hadoop3.tgz\n","\n","# Descomprimir la version de Spark\n","\n","!tar xf spark-3.3.4-bin-hadoop3.tgz\n","\n","# Establecer las variables de entorno\n","\n","import os\n","\n","os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n","os.environ[\"SPARK_HOME\"] = \"/content/spark-3.3.4-bin-hadoop3\"\n","\n","# Descargar findspark\n","\n","!pip install -q findspark\n","\n","# Crear la sesión de Spark\n","\n","import findspark\n","\n","findspark.init()\n","\n","from pyspark.sql import SparkSession\n","\n","spark = (\n","    SparkSession.builder\n","    .config('spark.ui.port', '4050')\n","    .getOrCreate()\n",")\n","\n","from google.colab import output\n","\n","output.serve_kernel_port_as_window(4050, path='/jobs/index.html')\n","\n","from pyspark.sql.functions import col\n","\n","spark.range(10000).toDF(\"id\").filter(col('id') / 2 == 0).write.mode('overwrite').parquet('/output')"]},{"cell_type":"markdown","metadata":{"id":"4xTFXrXNSG_b"},"source":["## Spark SQL Básico"]},{"cell_type":"markdown","metadata":{"id":"AONlSDOoxVza"},"source":["### Dataframes: fuentes de datos I"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UA7iE-ksSL38"},"outputs":[],"source":["# Creando DataFrames\n","\n","import findspark\n","findspark.init()\n","from pyspark.sql import SparkSession\n","\n","spark = SparkSession.builder.getOrCreate()\n","sc = spark.sparkContext\n","\n","rdd = sc.parallelize([item for item in range(10)]).map(lambda x: (x, x ** 2))\n","\n","rdd.collect()\n","\n","df = rdd.toDF(['numero', 'cudrado'])\n","\n","df.printSchema()\n","\n","df.show()\n","\n","# Crear un DataFrame a partir de un RDD con schema\n","\n","rdd1 = sc.parallelize([(1, 'Jose', 35.5), (2, 'Teresa', 54.3), (3, 'Katia', 12.7)])\n","\n","from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType\n","\n","# Primera vía\n","\n","esquema1 = StructType(\n","    [\n","     StructField('id', IntegerType(), True),\n","     StructField('nombre', StringType(), True),\n","     StructField('saldo', DoubleType(), True)\n","    ]\n",")\n","\n","# Segunda vía\n","\n","esquema2 = \"`id` INT, `nombre` STRING, `saldo` DOUBLE\"\n","\n","df1 = spark.createDataFrame(rdd1, schema=esquema1)\n","\n","df1.printSchema()\n","\n","df1.show()\n","\n","df2 = spark.createDataFrame(rdd1, schema=esquema2)\n","\n","df2.printSchema()\n","\n","df2.show()\n","\n","# Crear un DataFrame a partir de un rango de números\n","\n","spark.range(5).toDF('id').show()\n","\n","spark.range(3, 15).toDF('id').show()\n","\n","spark.range(0, 20, 2).toDF('id').show()\n","\n"]},{"cell_type":"markdown","metadata":{"id":"3HYKvYDfxkkF"},"source":["### Dataframes: fuentes de datos II"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vVtpG6-GSMYe"},"outputs":[],"source":["# Creando DataFrames\n","\n","import findspark\n","findspark.init()\n","from pyspark.sql import SparkSession\n","\n","spark = SparkSession.builder.getOrCreate()\n","sc = spark.sparkContext\n","\n","rdd = sc.parallelize([item for item in range(10)]).map(lambda x: (x, x ** 2))\n","\n","rdd.collect()\n","\n","df = rdd.toDF(['numero', 'cudrado'])\n","\n","df.printSchema()\n","\n","df.show()\n","\n","# Crear un DataFrame a partir de un RDD con schema\n","\n","rdd1 = sc.parallelize([(1, 'Jose', 35.5), (2, 'Teresa', 54.3), (3, 'Katia', 12.7)])\n","\n","from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType\n","\n","# Primera vía\n","\n","esquema1 = StructType(\n","    [\n","     StructField('id', IntegerType(), True),\n","     StructField('nombre', StringType(), True),\n","     StructField('saldo', DoubleType(), True)\n","    ]\n",")\n","\n","# Segunda vía\n","\n","esquema2 = \"`id` INT, `nombre` STRING, `saldo` DOUBLE\"\n","\n","df1 = spark.createDataFrame(rdd1, schema=esquema1)\n","\n","df1.printSchema()\n","\n","df1.show()\n","\n","df2 = spark.createDataFrame(rdd1, schema=esquema2)\n","\n","df2.printSchema()\n","\n","df2.show()\n","\n","# Crear un DataFrame a partir de un rango de números\n","\n","spark.range(5).toDF('id').show()\n","\n","spark.range(3, 15).toDF('id').show()\n","\n","spark.range(0, 20, 2).toDF('id').show()\n","\n"]},{"cell_type":"markdown","metadata":{"id":"NtTo8TzaIfoU"},"source":["### Trabajo con columnas"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DKZHSUcKSMWp"},"outputs":[],"source":["# Trabajo con columnas\n","\n","import findspark\n","findspark.init()\n","from pyspark.sql import SparkSession\n","\n","spark = SparkSession.builder.getOrCreate()\n","\n","df = spark.read.parquet('./data/dataPARQUET.parquet')\n","\n","df.printSchema()\n","\n","# Primera alternativa para referirnos a las columnas\n","\n","df.select('title').show()\n","\n","# Segunda alternativa\n","\n","from pyspark.sql.functions import col\n","\n","df.select(col('title')).show()\n","\n"]},{"cell_type":"markdown","metadata":{"id":"cs1KbFDTL08b"},"source":["### Transformaciones, funciones, select y selectExpr"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1uFizpl8SMU7"},"outputs":[],"source":["# Transformaciones - funciones select y selectExpr\n","\n","import findspark\n","findspark.init()\n","from pyspark.sql import SparkSession\n","\n","spark = SparkSession.builder.getOrCreate()\n","\n","# select\n","\n","df = spark.read.parquet('./data/datos.parquet')\n","\n","df.printSchema()\n","\n","from pyspark.sql.functions import col\n","\n","df.select(col('video_id')).show()\n","\n","df.select('video_id', 'trending_date').show()\n","\n","# Esta vía nos dará error\n","\n","df.select(\n","    'likes',\n","    'dislikes',\n","    ('likes' - 'dislikes')\n",").show()\n","\n","# Forma correcta\n","\n","df.select(\n","    col('likes'),\n","    col('dislikes'),\n","    (col('likes') - col('dislikes')).alias('aceptacion')\n",").show()\n","\n","# selectExpr\n","\n","df.selectExpr('likes', 'dislikes', '(likes - dislikes) as aceptacion').show()\n","\n","df.selectExpr(\"count(distinct(video_id)) as videos\").show()\n","\n"]},{"cell_type":"markdown","metadata":{"id":"xiAz20BnMJta"},"source":["### Transformaciones, funciones, filter y where"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"z6qA-SFaSMTF"},"outputs":[],"source":["# Transformaciones - funciones filter y where\n","\n","import findspark\n","findspark.init()\n","from pyspark.sql import SparkSession\n","\n","spark = SparkSession.builder.getOrCreate()\n","\n","df = spark.read.parquet('./data/datos.parquet')\n","\n","# filter\n","\n","from pyspark.sql.functions import col\n","\n","df.show()\n","\n","df.filter(col('video_id') == '2kyS6SvSYSE').show()\n","\n","df1 = spark.read.parquet('./data/datos.parquet').where(col('trending_date') != '17.14.11')\n","\n","df1.show()\n","\n","df2 = spark.read.parquet('./data/datos.parquet').where(col('likes') > 5000)\n","\n","df2.filter((col('trending_date') != '17.14.11') & (col('likes') > 7000)).show()\n","\n","df2.filter(col('trending_date') != '17.14.11').filter(col('likes') > 7000).show()\n"]},{"cell_type":"markdown","metadata":{"id":"p_oJWORyMh8U"},"source":["### Transformaciones, funciones, distinct y dropDuplicates"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8NiX0Qk2Mih5"},"outputs":[],"source":["# Transformaciones - funciones distinct y dropDuplicates\n","\n","import findspark\n","findspark.init()\n","from pyspark.sql import SparkSession\n","\n","spark = SparkSession.builder.getOrCreate()\n","\n","df = spark.read.parquet('./data')\n","\n","# distinct\n","\n","df_sin_duplicados = df.distinct()\n","\n","print('El conteo del dataframe original es {}'.format(df.count()))\n","print('El conteo del dataframe sin duplicados es {}'.format(df_sin_duplicados.count()))\n","\n","# función dropDuplicates\n","\n","dataframe = spark.createDataFrame([(1, 'azul', 567), (2, 'rojo', 487), (1, 'azul', 345), (2, 'verde', 783)]).toDF('id', 'color', 'importe')\n","\n","dataframe.show()\n","\n","dataframe.dropDuplicates(['id', 'color']).show()"]},{"cell_type":"markdown","metadata":{"id":"2LjNzIixMlyz"},"source":["### Transformaciones, funciones, sort y orderBy"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QuotBztUMmPH"},"outputs":[],"source":["# Transformaciones - funciones sort y orderBy\n","\n","import findspark\n","findspark.init()\n","from pyspark.sql import SparkSession\n","\n","spark = SparkSession.builder.getOrCreate()\n","\n","from pyspark.sql.functions import col\n","\n","df = (spark.read.parquet('./data')\n","    .select(col('likes'), col('views'), col('video_id'), col('dislikes'))\n","    .dropDuplicates(['video_id'])\n",")\n","\n","df.show()\n","\n","# sort\n","\n","df.sort('likes').show()\n","\n","from pyspark.sql.functions import desc\n","\n","df.sort(desc('likes')).show()\n","\n","# función orderBy\n","\n","df.orderBy(col('views')).show()\n","\n","df.orderBy(col('views').desc()).show()\n","\n","dataframe = spark.createDataFrame([(1, 'azul', 568), (2, 'rojo', 235), (1, 'azul', 456), (2, 'azul', 783)]).toDF('id', 'color', 'importe')\n","\n","dataframe.show()\n","\n","dataframe.orderBy(col('color').desc(), col('importe')).show()\n","\n","# funcion limit\n","\n","top_10 = df.orderBy(col('views').desc()).limit(10)\n","\n","top_10.show()\n"]},{"cell_type":"markdown","metadata":{"id":"SGlX53FJMmpp"},"source":["### Transformaciones, funciones, withColumn y withColumnRenamed"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"O9xWLWXBMn_-"},"outputs":[],"source":["# Transformaciones - funciones withColumn y withColumnRenamed\n","\n","import findspark\n","findspark.init()\n","from pyspark.sql import SparkSession\n","\n","spark = SparkSession.builder.getOrCreate()\n","\n","df = spark.read.parquet('./data')\n","\n","# withColumn\n","\n","from pyspark.sql.functions import col\n","\n","df_valoracion = df.withColumn('valoracion', col('likes') - col('dislikes'))\n","\n","df_valoracion.printSchema()\n","\n","df_valoracion1 = (df.withColumn('valoracion', col('likes') - col('dislikes'))\n","                    .withColumn('res_div', col('valoracion') % 10)\n",")\n","\n","df_valoracion1.printSchema()\n","\n","df_valoracion1.select(col('likes'), col('dislikes'), col('valoracion'), col('res_div')).show()\n","\n","# withColumnRenamed\n","\n","df_renombrado = df.withColumnRenamed('video_id', 'id')\n","\n","df_renombrado.printSchema()\n","\n","df_error = df.withColumnRenamed('nombre_que_no_existe', 'otro_nombre')\n","\n","df_error.printSchema()\n"]},{"cell_type":"markdown","metadata":{"id":"3xzrZ4O-ModN"},"source":["### Transformaciones, funciones, drop, sample, randomSplit"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lBlIb0SxMo7F"},"outputs":[],"source":["# Transformaciones - funciones drop, sample y randomSplit\n","\n","import findspark\n","findspark.init()\n","from pyspark.sql import SparkSession\n","\n","spark = SparkSession.builder.getOrCreate()\n","\n","df = spark.read.parquet('./data')\n","\n","# drop\n","\n","df.printSchema()\n","\n","df_util = df.drop('comments_disabled')\n","\n","df_util.printSchema()\n","\n","df_util = df.drop('comments_disabled', 'ratings_disabled', 'thumbnail_link')\n","\n","df_util.printSchema()\n","\n","df_util = df.drop('comments_disabled', 'ratings_disabled', 'thumbnail_link', 'cafe')\n","\n","df_util.printSchema()\n","\n","# sample\n","\n","df_muestra = df.sample(0.8)\n","\n","num_filas = df.count()\n","num_filas_muestra = df_muestra.count()\n","\n","print('El 80% de filas del dataframe original es {}'.format(num_filas - (num_filas*0.2)))\n","print('El numero de filas del dataframe muestra es {}'.format(num_filas_muestra))\n","\n","df_muestra = df.sample(fraction=0.8, seed=1234)\n","\n","df_muestra = df.sample(withReplacement=True, fraction=0.8, seed=1234)\n","\n","# randomSplit\n","\n","train, test = df.randomSplit([0.8, 0.2], seed=1234)\n","\n","train, validation, test = df.randomSplit([0.6, 0.2, 0.2], seed=1234)\n","\n","train.count()\n","\n","validation.count()\n","\n","test.count()\n"]},{"cell_type":"markdown","metadata":{"id":"rbFVOU_hMpeL"},"source":["Trabajo con datos incorrectos faltantes"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"I19kHepLMp-v"},"outputs":[],"source":["# Trabajo con datos incorrectos o faltantes\n","\n","import findspark\n","findspark.init()\n","from pyspark.sql import SparkSession\n","\n","spark = SparkSession.builder.getOrCreate()\n","\n","df = spark.read.parquet('./data/')\n","\n","df.count()\n","\n","df.na.drop().count()\n","\n","df.na.drop('any').count()\n","\n","df.dropna().count()\n","\n","df.na.drop(subset=['views']).count()\n","\n","df.na.drop(subset=['views', 'dislikes']).count()\n","\n","from pyspark.sql.functions import col\n","\n","df.orderBy(col('views')).select(col('views'), col('likes'), col('dislikes')).show()\n","\n","df.fillna(0).orderBy(col('views')).select(col('views'), col('likes'), col('dislikes')).show()\n","\n","df.fillna(0, subset=['likes', 'dislikes']).orderBy(col('views')).select(col('views'), col('likes'), col('dislikes')).show()\n"]},{"cell_type":"markdown","metadata":{"id":"7p6_VdbHMsQi"},"source":["### Acciones sobre un dataframe"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5tl1aFT8Mstx"},"outputs":[],"source":["# Acciones sobre un dataframe en Spark SQL\n","\n","import findspark\n","findspark.init()\n","from pyspark.sql import SparkSession\n","\n","spark = SparkSession.builder.getOrCreate()\n","\n","df = spark.read.parquet('./data/')\n","\n","# show\n","\n","df.show()\n","\n","df.show(5)\n","\n","df.show(5, truncate=False)\n","\n","# take\n","\n","df.take(1)\n","\n","# head\n","\n","df.head(1)\n","\n","# collect\n","\n","df.select('likes').collect()"]},{"cell_type":"markdown","metadata":{"id":"pKKo3Kt2MtSN"},"source":["### Escritura de dataframes"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8_bIFXbnMtxY"},"outputs":[],"source":["# Escritura de DataFrames\n","\n","import findspark\n","findspark.init()\n","from pyspark.sql import SparkSession\n","\n","spark = SparkSession.builder.getOrCreate()\n","\n","df = spark.read.parquet('./data/')\n","\n","df1 = df.repartition(2)\n","\n","df1.write.format('csv').option('sep', '|').save()\n","\n","df1.coalesce(1).write.format('csv').option('sep', '|').save('./output/csv1')\n","\n","df.printSchema()\n","\n","df.select('comments_disabled').distinct().show()\n","\n","from pyspark.sql.functions import col\n","\n","df_limpio = df.filter(col('comments_disabled').isin('True', 'False'))\n","\n","df_limpio.write.partitionBy('comments_disabled').parquet('./output/parquet')\n"]},{"cell_type":"markdown","metadata":{"id":"f4N3NCn5MuQx"},"source":["### Lectura 1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-8T8EdA4Muu1"},"outputs":[],"source":["# Instalar SDK java 8\n","\n","!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n","\n","# Descargar Spark\n","\n","!wget -q https://archive.apache.org/dist/spark/spark-3.3.4/spark-3.3.4-bin-hadoop3.tgz\n","\n","# Descomprimir la version de Spark\n","\n","!tar xf spark-3.3.4-bin-hadoop3.tgz\n","\n","# Establecer las variables de entorno\n","\n","import os\n","\n","os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n","os.environ[\"SPARK_HOME\"] = \"/content/spark-3.3.4-bin-hadoop3\"\n","\n","# Descargar findspark\n","\n","!pip install -q findspark\n","\n","# Instalar dotenv para manejar las credenciales\n","\n","!pip install python-dotenv\n","\n","# Extraer las credenciales del archivo .env a un diccionario de Python\n","\n","from dotenv import dotenv_values\n","\n","config = dotenv_values(\".env\")\n","\n","# Crear la sesión de Spark con las configuraciones necesarias para conectarse a AWS S3\n","\n","import findspark\n","\n","findspark.init()\n","\n","from pyspark.sql import SparkSession\n","\n","spark = (SparkSession\n","         .builder\n","         .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:3.3.1,com.amazonaws:aws-java-sdk-bundle:1.11.469\")\n","         .config(\"spark.hadoop.fs.s3a.aws.credentials.provider\", \"org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider\")\n","         .getOrCreate()\n","         )\n","\n","# Extraer las credenciales del diccionario\n","\n","accessKeyId=config.get('ACCESS_KEY')\n","secretAccessKey=config.get('SECRET_ACCESS_KEY')\n","\n","# Establecer las configuraciones de Hodoop necesarias\n","\n","sc = spark.sparkContext\n","\n","sc._jsc.hadoopConfiguration().set('fs.s3a.access.key', accessKeyId)\n","sc._jsc.hadoopConfiguration().set('fs.s3a.secret.key', secretAccessKey)\n","sc._jsc.hadoopConfiguration().set('fs.s3a.path.style.access', 'true')\n","sc._jsc.hadoopConfiguration().set('fs.s3a.impl', 'org.apache.hadoop.fs.s3a.S3AFileSystem')\n","sc._jsc.hadoopConfiguration().set('fs.s3a.endpoint', 's3.amazonaws.com')\n","\n","df = spark.read.parquet('s3a://josemtech/parquet')\n","\n","df.show()\n","\n","df1 = spark.read.option('header', 'true').option('inferSchema', 'true').csv('s3a://josemtech/csv/')\n","\n","df1.show()\n","\n","df.write.mode('overwrite').parquet('s3a://josemtech/salida')"]},{"cell_type":"markdown","metadata":{"id":"MvNwF1XnMvIV"},"source":["### Lectura 2"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"T89b3e_bMvnO"},"outputs":[],"source":["# Instalar SDK java 8\n","\n","!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n","\n","# Descargar Spark\n","\n","!wget -q https://archive.apache.org/dist/spark/spark-3.3.4/spark-3.3.4-bin-hadoop3.tgz\n","\n","# Descomprimir la version de Spark\n","\n","!tar xf spark-3.3.4-bin-hadoop3.tgz\n","\n","# Establecer las variables de entorno\n","\n","import os\n","\n","os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n","os.environ[\"SPARK_HOME\"] = \"/content/spark-3.3.4-bin-hadoop3\"\n","\n","# Descargar findspark\n","\n","!pip install -q findspark\n","\n","# Extraer las credenciales desde los Secrets\n","\n","from google.colab import userdata\n","\n","account_key = userdata.get('ACCOUNT_KEY')\n","\n","# Crear la sesión de Spark\n","\n","import findspark\n","\n","findspark.init()\n","\n","from pyspark.sql import SparkSession\n","\n","spark = (SparkSession.builder\n","         .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-azure:3.3.6,com.microsoft.azure:azure-storage:8.6.6\")\n","         .config(\"spark.hadoop.fs.azure.account.key.josemtech.blob.core.windows.net\", account_key)\n","         .config(\"spark.hadoop.fs.wasbs.impl\", \"org.apache.hadoop.fs.azure.NativeAzureFileSystem\")\n","         .config(\"spark.hadoop.fs.azure\", \"org.apache.hadoop.fs.azure.NativeAzureFileSystem\")\n","         .getOrCreate())\n","\n","# Luctura\n","\n","df = spark.read.parquet(\"wasbs://spark-data@josemtech.blob.core.windows.net/parquet\")\n","\n","df.show()\n","\n","# Escritura\n","\n","df.write.mode(\"overwrite\").parquet(\"wasbs://spark-data@josemtech.blob.core.windows.net/test/\")"]},{"cell_type":"markdown","metadata":{"id":"wButYQ9uMxQE"},"source":["### Lectura 3"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iyIJU9lWMxqp"},"outputs":[],"source":["# Instalar SDK java 8\n","\n","!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n","\n","# Descargar Spark\n","\n","!wget -q https://archive.apache.org/dist/spark/spark-3.3.4/spark-3.3.4-bin-hadoop3.tgz\n","\n","# Descomprimir la version de Spark\n","\n","!tar xf spark-3.3.4-bin-hadoop3.tgz\n","\n","# Establecer las variables de entorno\n","\n","import os\n","\n","os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n","os.environ[\"SPARK_HOME\"] = \"/content/spark-3.3.4-bin-hadoop3\"\n","\n","# Descargar findspark\n","\n","!pip install -q findspark\n","\n","# Descargar el jar necesario para conectarse al bucket de GCP\n","\n","!wget https://repo1.maven.org/maven2/com/google/cloud/bigdataoss/gcs-connector/hadoop3-2.2.9/gcs-connector-hadoop3-2.2.9-shaded.jar\n","\n","# Mover el jar descargado a la carpeta de jars de Spark\n","\n","!mv gcs-connector-hadoop3-2.2.9-shaded.jar /content/spark-3.4.2-bin-hadoop3/jars\n","\n","# Crear la sesión de Spark\n","\n","import findspark\n","\n","findspark.init()\n","\n","from pyspark.sql import SparkSession\n","\n","spark = (SparkSession.builder\n","         .config(\"spark.hadoop.fs.gs.impl\",\"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\")\n","         .config(\"google.cloud.auth.service.account.json.keyfile\",\"/content/pyspark.json\")\n","         .getOrCreate())\n","\n","df = spark.read.parquet('gs://josemtech/parquet')\n","\n","df.show()\n","\n","df.write.mode('overwrite').parquet('gs://josemtech/salida_parquet')\n","\n","df1 = spark.read.option('header', 'true').csv('gs://josemtech/csv')\n","\n","df1.show()\n","\n","df1.write.mode('overwrite').csv('gs://josemtech/salida_csv')"]},{"cell_type":"markdown","metadata":{"id":"UlhE480VMycw"},"source":["### Persistencia de dataframes"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mu6fact2My6s"},"outputs":[],"source":["# Persistencia de DataFrames\n","\n","import findspark\n","findspark.init()\n","from pyspark.sql import SparkSession\n","\n","spark = SparkSession.builder.getOrCreate()\n","\n","df = spark.createDataFrame([(1, 'a'), (2, 'b'), (3, 'c')], ['id', 'valor'])\n","\n","df.show()\n","\n","df.persist()\n","\n","df.unpersist()\n","\n","df.cache()\n","\n","from pyspark.storagelevel import StorageLevel\n","\n","df.persist(StorageLevel.DISK_ONLY)\n","\n","df.persist(StorageLevel.MEMORY_AND_DISK)\n"]},{"cell_type":"markdown","metadata":{"id":"P1wVBHtsMzTy"},"source":["### Ejercicios"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nfs86Gc5Mz8b"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"vw6VZXSpSNZA"},"source":["## Spark SQL Avanzado"]},{"cell_type":"markdown","metadata":{"id":"v-G812zxe7SN"},"source":[]},{"cell_type":"markdown","metadata":{"id":"KkpLftPxe3bN"},"source":["### Agregaciones"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"J8Pg82cqSUci"},"outputs":[],"source":["# Explorando los datos\n","\n","import findspark\n","findspark.init()\n","from pyspark.sql import SparkSession\n","\n","spark = SparkSession.builder.getOrCreate()\n","\n","df = spark.read.parquet('./data/')\n","\n","df.printSchema()\n","\n","df.show(20, truncate=False)\n"]},{"cell_type":"markdown","metadata":{"id":"0zJIVCCwe8Ba"},"source":["### Funciones"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"T3Z9A3yzSVUt"},"outputs":[],"source":["# Funciones count, countDistinct y approx_count_distinct\n","\n","import findspark\n","findspark.init()\n","from pyspark.sql import SparkSession\n","\n","spark = SparkSession.builder.getOrCreate()\n","\n","df = spark.read.parquet('./data/dataframe')\n","\n","df.printSchema()\n","\n","df.show()\n","\n","# count\n","\n","from pyspark.sql.functions import count\n","\n","df.select(\n","    count('nombre').alias('conteo_nombre'),\n","    count('color').alias('conteo_color')\n",").show()\n","\n","df.select(\n","    count('nombre').alias('conteo_nombre'),\n","    count('color').alias('conteo_color'),\n","    count('*').alias('conteo_general')\n",").show()\n","\n","# countDistinct\n","\n","from pyspark.sql.functions import countDistinct\n","\n","df.select(\n","    countDistinct('color').alias('colores_dif')\n",").show()\n","\n","# approx_count_distinct\n","\n","from pyspark.sql.functions import approx_count_distinct\n","\n","dataframe = spark.read.parquet('./data/vuelos')\n","\n","dataframe.printSchema()\n","\n","dataframe.select(\n","    countDistinct('AIRLINE'),\n","    approx_count_distinct('AIRLINE')\n",").show()\n"]},{"cell_type":"markdown","metadata":{"id":"g1z5xso0e90P"},"source":["### Funciones min y max"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s0B2ZOlCSVTB"},"outputs":[],"source":["# Funciones min y max\n","\n","import findspark\n","findspark.init()\n","from pyspark.sql import SparkSession\n","\n","spark = SparkSession.builder.getOrCreate()\n","\n","vuelos = spark.read.parquet('./data')\n","\n","vuelos.printSchema()\n","\n","from pyspark.sql.functions import min, max, col\n","\n","vuelos.select(\n","    min('AIR_TIME').alias('menor_timepo'),\n","    max('AIR_TIME').alias('mayor_tiempo')\n",").show()\n","\n","vuelos.select(\n","    min('AIRLINE_DELAY'),\n","    max('AIRLINE_DELAY')\n",").show()\n"]},{"cell_type":"markdown","metadata":{"id":"p4Qqkqe5fBhq"},"source":["### Funciones sum y sumDistinct"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"v1xQbT4kSVRG"},"outputs":[],"source":["# Funciones sum, sumDistinct y avg\n","\n","import findspark\n","findspark.init()\n","from pyspark.sql import SparkSession\n","\n","spark = SparkSession.builder.getOrCreate()\n","\n","vuelos = spark.read.parquet('./data/')\n","\n","from pyspark.sql.functions import sum, sumDistinct, avg, count\n","\n","# sum\n","\n","vuelos.printSchema()\n","\n","vuelos.select(\n","    sum('DISTANCE').alias('sum_dis')\n",").show()\n","\n","# sumDistinct\n","\n","vuelos.select(\n","    sumDistinct('DISTANCE').alias('sum_dis_dif')\n",").show()\n","\n","# avg\n","\n","vuelos.select(\n","    avg('AIR_TIME').alias('promedio_aire'),\n","    (sum('AIR_TIME') / count('AIR_TIME')).alias('prom_manual')\n",").show()\n"]},{"cell_type":"markdown","metadata":{"id":"sEqgOplGf6rT"},"source":["### Agregación con agrupación"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ToBOY8kQTIQt"},"outputs":[],"source":["# Agregación con agrupación\n","\n","import findspark\n","findspark.init()\n","from pyspark.sql import SparkSession\n","\n","spark = SparkSession.builder.getOrCreate()\n","\n","vuelos = spark.read.parquet('./data/')\n","\n","vuelos.printSchema()\n","\n","from pyspark.sql.functions import desc\n","\n","(vuelos.groupBy('ORIGIN_AIRPORT')\n","    .count()\n","    .orderBy(desc('count'))\n",").show()\n","\n","(vuelos.groupBy('ORIGIN_AIRPORT', 'DESTINATION_AIRPORT')\n","    .count()\n","    .orderBy(desc('count'))\n",").show()\n"]},{"cell_type":"markdown","metadata":{"id":"CWDte3lOf-KC"},"source":["### Varias agregaciones por grupo"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bSSxwk-hTIOY"},"outputs":[],"source":["# Varias agregaciones por grupo\n","\n","import findspark\n","findspark.init()\n","from pyspark.sql import SparkSession\n","\n","spark = SparkSession.builder.getOrCreate()\n","\n","vuelos = spark.read.parquet('./data/')\n","\n","from pyspark.sql.functions import count, min, max, desc, avg\n","\n","vuelos.groupBy('ORIGIN_AIRPORT').agg(\n","    count('AIR_TIME').alias('tiempo_aire'),\n","    min('AIR_TIME').alias('min'),\n","    max('AIR_TIME').alias('max')\n",").orderBy(desc('tiempo_aire')).show()\n","\n","vuelos.groupBy('MONTH').agg(\n","    count('ARRIVAL_DELAY').alias('conteo_de_retrasos'),\n","    avg('DISTANCE').alias('prom_dist')\n",").orderBy(desc('conteo_de_retrasos')).show()\n"]},{"cell_type":"markdown","metadata":{"id":"YiptC7kTgCM6"},"source":["### Agregación con pivote"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6YZx8tb1TIME"},"outputs":[],"source":["# Agregación con pivote\n","\n","import findspark\n","findspark.init()\n","from pyspark.sql import SparkSession\n","\n","spark = SparkSession.builder.getOrCreate()\n","\n","estudiantes = spark.read.parquet('./data/')\n","\n","estudiantes.show()\n","\n","from pyspark.sql.functions import min, max, avg, col\n","\n","estudiantes.groupBy('graduacion').pivot('sexo').agg(avg('peso')).show()\n","\n","estudiantes.groupBy('graduacion').pivot('sexo').agg(avg('peso'), min('peso'), max('peso')).show()\n","\n","estudiantes.groupBy('graduacion').pivot('sexo', ['M']).agg(avg('peso'), min('peso'), max('peso')).show()\n","\n","estudiantes.groupBy('graduacion').pivot('sexo', ['F']).agg(avg('peso'), min('peso'), max('peso')).show()\n"]},{"cell_type":"markdown","metadata":{"id":"cLpY1rmEgPet"},"source":["### Inner Join"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_waX_gIIgP3q"},"outputs":[],"source":["# Inner Join\n","\n","import findspark\n","findspark.init()\n","from pyspark.sql import SparkSession\n","\n","spark = SparkSession.builder.getOrCreate()\n","\n","empleados = spark.read.parquet('./data/empleados')\n","\n","departamentos = spark.read.parquet('./data/departamentos')\n","\n","empleados.show()\n","\n","departamentos.show()\n","\n","# Inner join\n","\n","from pyspark.sql.functions import col\n","\n","join_df = empleados.join(departamentos, col('num_dpto') == col('id'))\n","\n","join_df.show()\n","\n","join_df = empleados.join(departamentos, col('num_dpto') == col('id'), 'inner')\n","\n","join_df.show()\n","\n","join_df = empleados.join(departamentos).where(col('num_dpto') == col('id'))\n","\n","join_df.show()\n"]},{"cell_type":"markdown","metadata":{"id":"7KfvGnhbgQZy"},"source":["### Left Outer Join"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fT6syaongQuo"},"outputs":[],"source":["# Left Outer Join\n","\n","import findspark\n","findspark.init()\n","from pyspark.sql import SparkSession\n","\n","spark = SparkSession.builder.getOrCreate()\n","\n","empleados = spark.read.parquet('./data/empleados/')\n","\n","departamentos = spark.read.parquet('./data/departamentos/')\n","\n","from pyspark.sql.functions import col\n","\n","empleados.join(departamentos, col('num_dpto') == col('id'), 'leftouter').show()\n","\n","empleados.join(departamentos, col('num_dpto') == col('id'), 'left_outer').show()\n","\n","empleados.join(departamentos, col('num_dpto') == col('id'), 'left').show()\n"]},{"cell_type":"markdown","metadata":{"id":"yw4AEEhRgRJ8"},"source":["### Right Outer Join"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MPV0x9RKgRie"},"outputs":[],"source":["# Right Outer Join\n","\n","import findspark\n","findspark.init()\n","from pyspark.sql import SparkSession\n","\n","spark = SparkSession.builder.getOrCreate()\n","\n","empleados = spark.read.parquet('./data/empleados/')\n","\n","departamentos = spark.read.parquet('./data/departamentos/')\n","\n","from pyspark.sql.functions import col\n","\n","empleados.join(departamentos, col('num_dpto') == col('id'), 'rightouter').show()\n","\n","empleados.join(departamentos, col('num_dpto') == col('id'), 'right_outer').show()\n","\n","empleados.join(departamentos, col('num_dpto') == col('id'), 'right').show()\n"]},{"cell_type":"markdown","metadata":{"id":"_9ZcZL6XgfSK"},"source":["### Full Outer Join"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"S20OuGOfgfm9"},"outputs":[],"source":["# Full Outer Join\n","\n","import findspark\n","findspark.init()\n","from pyspark.sql import SparkSession\n","\n","spark = SparkSession.builder.getOrCreate()\n","\n","empleados = spark.read.parquet('./data/empleados/')\n","\n","departamentos = spark.read.parquet('./data/departamentos/')\n","\n","from pyspark.sql.functions import col\n","\n","empleados.join(departamentos, col('num_dpto') == col('id'), 'outer').show()\n"]},{"cell_type":"markdown","metadata":{"id":"8J-s9XH4ggHo"},"source":["### Left Anti Join"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mgjIJmo7gglZ"},"outputs":[],"source":["# Left Anti Join\n","\n","import findspark\n","findspark.init()\n","from pyspark.sql import SparkSession\n","\n","spark = SparkSession.builder.getOrCreate()\n","\n","empleados = spark.read.parquet('./data/empleados/')\n","\n","departamentos = spark.read.parquet('./data/departamentos/')\n","\n","from pyspark.sql.functions import col\n","\n","empleados.join(departamentos, col('num_dpto') == col('id'), 'left_anti').show()\n","\n","departamentos.join(empleados, col('num_dpto') == col('id'), 'left_anti').show()\n"]},{"cell_type":"markdown","metadata":{"id":"sEmAsVQvgiLr"},"source":["### Left Semi Join"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UcQ0Xsj1gjCc"},"outputs":[],"source":["# Left Semi Join\n","\n","import findspark\n","findspark.init()\n","from pyspark.sql import SparkSession\n","\n","spark = SparkSession.builder.getOrCreate()\n","\n","empleados = spark.read.parquet('./data/empleados/')\n","\n","departamentos = spark.read.parquet('./data/departamentos/')\n","\n","from pyspark.sql.functions import col\n","\n","empleados.join(departamentos, col('num_dpto') == col('id'), 'left_semi').show()\n"]},{"cell_type":"markdown","metadata":{"id":"vvfgFSC9gjao"},"source":["### Cross Join"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"di9GHBeFgjzD"},"outputs":[],"source":["# Cross Join\n","\n","import findspark\n","findspark.init()\n","from pyspark.sql import SparkSession\n","\n","spark = SparkSession.builder.getOrCreate()\n","\n","empleados = spark.read.parquet('./data/empleados/')\n","\n","departamentos = spark.read.parquet('./data/departamentos/')\n","\n","from pyspark.sql.functions import col\n","\n","df = empleados.crossJoin(departamentos)\n","\n","df.show()\n","\n","df.count()"]},{"cell_type":"markdown","metadata":{"id":"DBCbKPrZgkJh"},"source":["### Manejo de nombres de columnas duplicados"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FncyxGK5gk32"},"outputs":[],"source":["# Manejo de nombres de columnas duplicados\n","\n","import findspark\n","findspark.init()\n","from pyspark.sql import SparkSession\n","\n","spark = SparkSession.builder.getOrCreate()\n","\n","empleados = spark.read.parquet('./data/empleados/')\n","\n","departamentos = spark.read.parquet('./data/departamentos/')\n","\n","from pyspark.sql.functions import col\n","\n","depa = departamentos.withColumn('num_dpto', col('id'))\n","\n","depa.printSchema()\n","\n","empleados.printSchema()\n","\n","# Devuelve un error\n","\n","empleados.join(depa, col('num_dpto') == col('num_dpto'))\n","\n","# Forma correcta\n","\n","df_con_duplicados = empleados.join(depa, empleados['num_dpto'] == depa['num_dpto'])\n","\n","df_con_duplicados.printSchema()\n","\n","df_con_duplicados.select(empleados['num_dpto']).show()\n","\n","df2 = empleados.join(depa, 'num_dpto')\n","\n","df2.printSchema()\n","\n","empleados.join(depa, ['num_dpto']).printSchema()\n"]},{"cell_type":"markdown","metadata":{"id":"7-9rwntjglSm"},"source":["### Shuffle, hash join y broadcast hash join"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MM0cQYctglpe"},"outputs":[],"source":["# Shuffle Hash Join y Broadcast Hash Join\n","\n","import findspark\n","findspark.init()\n","from pyspark.sql import SparkSession\n","\n","spark = SparkSession.builder.getOrCreate()\n","\n","empleados = spark.read.parquet('./data/empleados/')\n","\n","departamentos = spark.read.parquet('./data/departamentos/')\n","\n","from pyspark.sql.functions import col, broadcast\n","\n","empleados.join(broadcast(departamentos), col('num_dpto') == col('id')).show()\n","\n","empleados.join(broadcast(departamentos), col('num_dpto') == col('id')).explain()"]},{"cell_type":"markdown","metadata":{"id":"oFfGOHPVgmP_"},"source":["### Ejercicios"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iqcG8TJcgmlh"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"NYsFpayISwks"},"source":["## Funciones en Spark SQL"]},{"cell_type":"markdown","metadata":{"id":"hQWmggGTj9cD"},"source":[]},{"cell_type":"markdown","metadata":{"id":"Me9ESk0Pj-vT"},"source":["### Funciones de fecha y hora"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8nRvqo0oSx23"},"outputs":[],"source":["# Funciones de fecha y hora\n","\n","import findspark\n","findspark.init()\n","from pyspark.sql import SparkSession\n","\n","spark = SparkSession.builder.getOrCreate()\n","\n","data = spark.read.parquet('./data/convertir')\n","\n","data.printSchema()\n","\n","data.show(truncate=False)\n","\n","from pyspark.sql.functions import col, to_date, to_timestamp\n","\n","data1 = data.select(\n","    to_date(col('date')).alias('date1'),\n","    to_timestamp(col('timestamp')).alias('ts1'),\n","    to_date(col('date_str'), 'dd-MM-yyyy').alias('date2'),\n","    to_timestamp(col('ts_str'), 'dd-MM-yyyy mm:ss').alias('ts2')\n","\n",")\n","\n","data1.show(truncate=False)\n","\n","data1.printSchema()\n","\n","from pyspark.sql.functions import date_format\n","\n","data1.select(\n","    date_format(col('date1'), 'dd-MM-yyyy')\n",").show()\n","\n","df = spark.read.parquet('./data/calculo')\n","\n","df.show()\n","\n","from pyspark.sql.functions import datediff, months_between, last_day\n","\n","df.select(\n","    col('nombre'),\n","    datediff(col('fecha_salida'), col('fecha_ingreso')).alias('dias'),\n","    months_between(col('fecha_salida'), col('fecha_ingreso')).alias('meses'),\n","    last_day(col('fecha_salida')).alias('ultimo_dia_mes')\n",").show()\n","\n","from pyspark.sql.functions import date_add, date_sub\n","\n","df.select(\n","    col('nombre'),\n","    col('fecha_ingreso'),\n","    date_add(col('fecha_ingreso'), 14).alias('mas_14_dias'),\n","    date_sub(col('fecha_ingreso'), 1).alias('menos_1_dia')\n",").show()\n","\n","from pyspark.sql.functions import year, month, dayofmonth, dayofyear, hour, minute, second\n","\n","df.select(\n","    col('baja_sistema'),\n","    year(col('baja_sistema')),\n","    month(col('baja_sistema')),\n","    dayofmonth(col('baja_sistema')),\n","    dayofyear(col('baja_sistema')),\n","    hour(col('baja_sistema')),\n","    minute(col('baja_sistema')),\n","    second(col('baja_sistema'))\n",").show()"]},{"cell_type":"markdown","metadata":{"id":"b-_XCfLdj-OR"},"source":["### Funciones de trabajo con Strings"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZsF10m6aSytz"},"outputs":[],"source":["# Funciones para trabajo con strings\n","\n","import findspark\n","findspark.init()\n","from pyspark.sql import SparkSession\n","\n","spark = SparkSession.builder.getOrCreate()\n","\n","data = spark.read.parquet('./data/')\n","\n","data.show()\n","\n","from pyspark.sql.functions import ltrim, rtrim, trim\n","\n","data.select(\n","    ltrim('nombre').alias('ltrim'),\n","    rtrim('nombre').alias('rtrim'),\n","    trim('nombre').alias('trim')\n",").show()\n","\n","from pyspark.sql.functions import col, lpad, rpad\n","\n","data.select(\n","    trim(col('nombre')).alias('trim')\n",").select(\n","    lpad(col('trim'), 8, '-').alias('lpad'),\n","    rpad(col('trim'), 8, '=').alias('rpad')\n",").show()\n","\n","df1 = spark.createDataFrame([('Spark', 'es', 'maravilloso')], ['sujeto', 'verbo', 'adjetivo'])\n","\n","df1.show()\n","\n","from pyspark.sql.functions import concat_ws, lower, upper, initcap, reverse\n","\n","df1.select(\n","    concat_ws(' ', col('sujeto'), col('verbo'), col('adjetivo')).alias('frase')\n",").select(\n","    col('frase'),\n","    lower(col('frase')).alias('minuscula'),\n","    upper(col('frase')).alias('mayuscula'),\n","    initcap(col('frase')).alias('initcap'),\n","    reverse(col('frase')).alias('reversa')\n",").show()\n","\n","from pyspark.sql.functions import regexp_replace\n","\n","df2 = spark.createDataFrame([(' voy a casa por mis llaves',)], ['frase'])\n","\n","df2.show(truncate=False)\n","\n","df2.select(\n","    regexp_replace(col('frase'), 'voy|por', 'ir').alias('nueva_frase')\n",").show(truncate=False)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"HW-lQGZnkLLX"},"source":["### Funciones de trabajo con colecciones"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JZHKnPIESyqw"},"outputs":[],"source":["# Funciones para trabajo con colecciones\n","\n","import findspark\n","findspark.init()\n","from pyspark.sql import SparkSession\n","\n","spark = SparkSession.builder.getOrCreate()\n","\n","data = spark.read.parquet('./data/parquet/')\n","\n","data.show(truncate=False)\n","\n","data.printSchema()\n","\n","from pyspark.sql.functions import col, size, sort_array, array_contains\n","\n","data.select(\n","    size(col('tareas')).alias('tamaño'),\n","    sort_array(col('tareas')).alias('arreglo_ordenado'),\n","    array_contains(col('tareas'), 'buscar agua').alias('buscar_agua')\n",").show(truncate=False)\n","\n","from pyspark.sql.functions import explode\n","\n","data.select(\n","    col('dia'),\n","    explode(col('tareas')).alias('tareas')\n",").show()\n","\n","# Formato JSON\n","\n","json_df_str = spark.read.parquet('./data/JSON')\n","\n","json_df_str.show(truncate=False)\n","\n","json_df_str.printSchema()\n","\n","from pyspark.sql.types import StructType, StructField, StringType, ArrayType\n","\n","schema_json = StructType(\n","    [\n","     StructField('dia', StringType(), True),\n","     StructField('tareas', ArrayType(StringType()), True)\n","    ]\n",")\n","\n","from pyspark.sql.functions import from_json, to_json\n","\n","json_df = json_df_str.select(\n","    from_json(col('tareas_str'), schema_json).alias('por_hacer')\n",")\n","\n","json_df.printSchema()\n","\n","json_df.select(\n","    col('por_hacer').getItem('dia'),\n","    col('por_hacer').getItem('tareas'),\n","    col('por_hacer').getItem('tareas').getItem(0).alias('primer_tarea')\n",").show(truncate=False)\n","\n","json_df.select(\n","    to_json(col('por_hacer'))\n",").show(truncate=False)"]},{"cell_type":"markdown","metadata":{"id":"PMFx1J3DkOr3"},"source":["### Funciones when, coalesce y lit"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GgL2eHkuTGjR"},"outputs":[],"source":["# Funciones when, coalesce y lit\n","\n","import findspark\n","findspark.init()\n","from pyspark.sql import SparkSession\n","\n","spark = SparkSession.builder.getOrCreate()\n","\n","data = spark.read.parquet('./data/')\n","\n","data.show()\n","\n","from pyspark.sql.functions import col, when, lit, coalesce\n","\n","data.select(\n","    col('nombre'),\n","    when(col('pago') == 1, 'pagado').when(col('pago') == 2, 'sin pagar').otherwise('sin iniciar').alias('pago')\n",").show()\n","\n","data.select(\n","    coalesce(col('nombre'), lit('sin nombre')).alias('nombre')\n",").show()"]},{"cell_type":"markdown","metadata":{"id":"XXN17SfvkX3i"},"source":["### Funciones definidas por el usuario UDF"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"k8uBs6wtTGgb"},"outputs":[],"source":["# Funciones definidas por el usuario UDF\n","\n","import findspark\n","findspark.init()\n","from pyspark.sql import SparkSession\n","\n","spark = SparkSession.builder.getOrCreate()\n","\n","def cubo(n):\n","    return n * n * n\n","\n","from pyspark.sql.types import LongType\n","\n","spark.udf.register('cubo', f_cubo, LongType())\n","\n","spark.range(1,10).createOrReplaceTempView('df_temp')\n","\n","spark.sql(\"SELECT id, cubo(id) AS cubo FROM df_temp\").show()\n","\n","def bienvenida(nombre):\n","    return ('Hola {}'.format(nombre))\n","\n","from pyspark.sql.functions import udf\n","from pyspark.sql.types import StringType\n","\n","bienvenida_udf = udf(lambda x: bienvenida(x), StringType())\n","\n","df_nombre = spark.createDataFrame([('Jose',), ('Julia',)], ['nombre'])\n","\n","df_nombre.show()\n","\n","from pyspark.sql.functions import col\n","\n","df_nombre.select(\n","    col('nombre'),\n","    bienvenida_udf(col('nombre')).alias('bie_nombre')\n",").show()\n","\n","@udf(returnType=StringType())\n","def mayuscula(s):\n","    return s.upper()\n","\n","df_nombre.select(\n","    col('nombre'),\n","    mayuscula(col('nombre')).alias('may_nombre')\n",").show()\n","\n","import pandas as pd\n","\n","from pyspark.sql.functions import pandas_udf\n","\n","def cubo_pandas(a: pd.Series) -> pd.Series:\n","    return a * a * a\n","\n","cubo_udf = pandas_udf(cubo_pandas, returnType=LongType())\n","\n","x = pd.Series([1, 2, 3])\n","\n","print(cubo_pandas(x))\n","\n","df = spark.range(5)\n","\n","df.select(\n","    col('id'),\n","    cubo_udf(col('id')).alias('cubo_pandas')\n",").show()"]},{"cell_type":"markdown","metadata":{"id":"Stw0QEHRkdJ_"},"source":["### Funciones de ventana"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CxtJs-cSTGdy"},"outputs":[],"source":["# Funciones de ventana\n","\n","import findspark\n","findspark.init()\n","from pyspark.sql import SparkSession\n","\n","spark = SparkSession.builder.getOrCreate()\n","\n","df = spark.read.parquet('./data/')\n","\n","df.show()\n","\n","from pyspark.sql.window import Window\n","from pyspark.sql.functions import desc, row_number, rank, dense_rank, col\n","\n","windowSpec = Window.partitionBy('departamento').orderBy(desc('evaluacion'))\n","\n","# row_number\n","\n","df.withColumn('row_number', row_number().over(windowSpec)).filter(col('row_number').isin(1,2)).show()\n","\n","# rank\n","\n","df.withColumn('rank', rank().over(windowSpec)).show()\n","\n","# dense_rank\n","\n","df.withColumn('dense_rank', dense_rank().over(windowSpec)).show()\n","\n","# Agregaciones con especificaciones de ventana\n","\n","windowSpecAgg = Window.partitionBy('departamento')\n","\n","from pyspark.sql.functions import min, max, avg\n","\n","(df.withColumn('min', min(col('evaluacion')).over(windowSpecAgg))\n",".withColumn('max', max(col('evaluacion')).over(windowSpecAgg))\n",".withColumn('avg', avg(col('evaluacion')).over(windowSpecAgg))\n",".withColumn('row_number', row_number().over(windowSpec))\n"," ).show()"]},{"cell_type":"markdown","metadata":{"id":"f9PUeF-mkf9G"},"source":["### Catalyst Optimizer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OvwuWD4ASyok"},"outputs":[],"source":["# Catalyst Optimizer\n","\n","import findspark\n","findspark.init()\n","from pyspark.sql import SparkSession\n","\n","spark = SparkSession.builder.getOrCreate()\n","\n","data = spark.read.parquet('./data/')\n","\n","data.printSchema()\n","\n","data.show()\n","\n","from pyspark.sql.functions import col\n","\n","nuevo_df = (data.filter(col('MONTH').isin(6,7,8))\n","            .withColumn('dis_tiempo_aire', col('DISTANCE') / col('AIR_TIME'))\n",").select(\n","    col('AIRLINE'),\n","    col('dis_tiempo_aire')\n",").where(col('AIRLINE').isin('AA', 'DL', 'AS'))\n","\n","nuevo_df.explain(True)"]},{"cell_type":"markdown","metadata":{"id":"vTojhBfqkktY"},"source":["### Ejercicios"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Jl5LPyxtklJz"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"d93Xay9kA95I"},"source":["-----------\n","## Fin Notebook\n","-----------"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyNFzE4V6asI5mWfMfnmxvbU","collapsed_sections":["_zJeZZqpJu73","LccNQX8WMBtl","4xTFXrXNSG_b","vw6VZXSpSNZA"],"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
