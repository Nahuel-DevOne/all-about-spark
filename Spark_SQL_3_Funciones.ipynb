{"cells":[{"cell_type":"markdown","metadata":{"id":"IhqAQ_OOKTXl"},"source":["# **Spark SQL Funciones**"]},{"cell_type":"markdown","metadata":{"id":"_zJeZZqpJu73"},"source":["## Introducción"]},{"cell_type":"markdown","metadata":{"id":"Ojkkv5J5rwg-"},"source":["### `Ventajas y desventajas de trabajar con Spark en Google Colab`"]},{"cell_type":"markdown","metadata":{"id":"AzJLfwssmyU6"},"source":["Ventajas:\n","- Fácil acceso\n","- Ejecutar Spark en prácticamente cualquier dispositivo, los recursos están en la nube.\n","- Como los recursos están la nube, no hay que preocuparse por los recursos de hardware\n","- Trabajo en equipo, más sencillo el trabajo colaborativo. Varias personas pueden trabajar sobre un mismo notebook.\n","\n","\n","Desventajas:\n","- No se guardan las configuraciones de Spark luego de un tiempo\n","> No obstante el notebook permanece intacto. Se puede volver a ejecutar las líneas de código para tener la configuración nuevamente.\n","- Escalabilidad, como el servicio es gratuito, los recursos son limitados.\n","> Para llevarlo a ambientes productivos, necesitamos una infraestructura capaz de brindarnos estas especificaciones."]},{"cell_type":"markdown","metadata":{"id":"CqjAFIHMJoQc"},"source":["## Instalaciones Necesarias para trabajar con Spark en Colab"]},{"cell_type":"markdown","metadata":{"id":"LccNQX8WMBtl"},"source":["### `Descarga e instalación de Apache Spark en Colab`\n","Se explica celda por celda las instalaciones necesarias. Para fines prácticos, utilizar la celda de abajo que instala todo junto."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JPNdekO7KHTp"},"outputs":[],"source":["# Instalar SDK Java 8\n","!apt-get install openjdk-8-jdk-headless -qq > /dev/null"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"peqw7BfRKjHE"},"outputs":[],"source":["# Descargar Spark 3.2.4\n","!wget -q https://archive.apache.org/dist/spark/spark-3.2.4/spark-3.2.4-bin-hadoop3.2.tgz"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wIsOwRkJKjEi"},"outputs":[],"source":["# Descomprimir el archivo descargado de Spark\n","!tar xf spark-3.2.4-bin-hadoop3.2.tgz"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sUtXLbwuKjBH"},"outputs":[],"source":["# Establecer las variables de entorno\n","import os\n","\n","os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n","os.environ[\"SPARK_HOME\"] = \"/content/spark-3.2.4-bin-hadoop3.2\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oMM5SJEPKi_X"},"outputs":[],"source":["# Instalar la librería findspark\n","!pip install -q findspark"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":36078,"status":"ok","timestamp":1708225550840,"user":{"displayName":"Nahuel Lopez","userId":"06859695819217714267"},"user_tz":180},"id":"51AZCLcVKi9x","outputId":"f9126ed1-b460-4e91-ea89-35223d84a511"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.9/316.9 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n"]}],"source":["# Instalar pyspark\n","!pip install -q pyspark"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2AMEAXrbKi8o"},"outputs":[],"source":["### verificar la instalación ###\n","import findspark\n","findspark.init()\n","\n","from pyspark.sql import SparkSession\n","spark = SparkSession.builder.master(\"local[*]\").getOrCreate()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6201,"status":"ok","timestamp":1708226374480,"user":{"displayName":"Nahuel Lopez","userId":"06859695819217714267"},"user_tz":180},"id":"QivCkbKRKi6g","outputId":"09e383c5-012f-421b-bb97-d329a12167b4"},"outputs":[{"name":"stdout","output_type":"stream","text":["+-----+\n","| Hola|\n","+-----+\n","|Mundo|\n","|Mundo|\n","|Mundo|\n","|Mundo|\n","|Mundo|\n","|Mundo|\n","|Mundo|\n","|Mundo|\n","|Mundo|\n","|Mundo|\n","+-----+\n","\n"]}],"source":["# Probando la sesión de Spark\n","df = spark.createDataFrame([{\"Hola\": \"Mundo\"} for x in range(10)])\n","# df.show(10, False)\n","df.show()"]},{"cell_type":"markdown","metadata":{"id":"U_xdI5-TOhbo"},"source":["### `Descarga e instalación de Apache Spark en una sola celda (Utilizar esta opción)`\n","Para fines prácticos, toda la instalación está en una celda, así luego de ejecutarse ya se puede trabajar con Spark."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":129065,"status":"ok","timestamp":1723487353125,"user":{"displayName":"Nahuel Lopez","userId":"06859695819217714267"},"user_tz":180},"id":"wEvprYN0Ot1l","outputId":"5cb64e13-1347-4f2b-d4c6-c7e3c9420b35"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.3/317.3 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n","+-----+\n","| Hola|\n","+-----+\n","|Mundo|\n","|Mundo|\n","|Mundo|\n","|Mundo|\n","|Mundo|\n","|Mundo|\n","|Mundo|\n","|Mundo|\n","|Mundo|\n","|Mundo|\n","+-----+\n","\n"]}],"source":["# Instalar SDK Java 8\n","!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n","\n","# Descargar Spark 3.2.4\n","!wget -q https://archive.apache.org/dist/spark/spark-3.2.4/spark-3.2.4-bin-hadoop3.2.tgz\n","\n","# Descomprimir el archivo descargado de Spark\n","!tar xf spark-3.2.4-bin-hadoop3.2.tgz\n","\n","# Establecer las variables de entorno\n","import os\n","\n","os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n","os.environ[\"SPARK_HOME\"] = \"/content/spark-3.2.4-bin-hadoop3.2\"\n","\n","# Instalar la librería findspark\n","!pip install -q findspark\n","\n","# Instalar pyspark\n","!pip install -q pyspark\n","\n","### verificar la instalación ###\n","import findspark\n","findspark.init()\n","\n","from pyspark.sql import SparkSession\n","spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n","\n","# Probando la sesión de Spark\n","df = spark.createDataFrame([{\"Hola\": \"Mundo\"} for x in range(10)])\n","# df.show(10, False)\n","df.show()"]},{"cell_type":"markdown","source":["## `Importante: Carga de archivos en Google Colab`\n","\n","Dos formas de cargar los archivos para resolver los ejercicios:\n","\n","1. **Montando el drive para acceder a los contenidos** de la unidad.\n","\n","  Utilizar esta opción si los datos para los ejercicios se cargan en una carpeta del drive y se quiere acceder a ella.\n","\n","2. **Utilizando el cuadro de archivos**, donde se carga el archivo que se quiere trabajar. Se guarda temporalmente.\n","> Esta es la forma que voy a estar utizando"],"metadata":{"id":"Jz_8wAjF-CZL"}},{"cell_type":"code","source":["# Levantar una sesión de Spark\n","import findspark\n","findspark.init()\n","from pyspark.sql import SparkSession\n","\n","spark = SparkSession.builder.appName('Cap2').master('local(*)').getOrCreate()\n","spark"],"metadata":{"id":"f-J8eWCz-Fei"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 1. Utilizando el montado al drive y yendo hacia la carpeta donde se encuentra el archivo."],"metadata":{"id":"QXy8YNeO-Jau"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"pmN9OdbH-KM9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["rdd_texto = sc.wholeTextFiles('/content/drive/MyDrive/Spark/data-ej-PySpark-RDD/el_valor_del_big_data.txt')\n","rdd_texto.collect()"],"metadata":{"id":"o79J5JOS-QN2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 2. Utilizando el cuadro de archivos, donde se carga el archivo que se quiere trabajar. Se guarda temporalmente.\n","\n","Esta es la que voy a estar utizando a lo largo de todos los notebooks.\n"],"metadata":{"id":"NAHsX9e2-Kxs"}},{"cell_type":"code","source":["rdd_texto = sc.wholeTextFiles('./el_valor_del_big_data.txt')\n","rdd_texto.collect()"],"metadata":{"id":"Lr2_B-R0-LeE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"q5eOVSiKAhWl"},"source":["## Spark UI en Colab"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"H1IQB2NmAleg","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1722544439913,"user_tz":180,"elapsed":49948,"user":{"displayName":"Nahuel Lopez","userId":"06859695819217714267"}},"outputId":"1dddf7c5-10f6-4930-bafe-4ccec733b3be"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["(async (port, path, text, element) => {\n","    if (!google.colab.kernel.accessAllowed) {\n","      return;\n","    }\n","    element.appendChild(document.createTextNode(''));\n","    const url = await google.colab.kernel.proxyPort(port);\n","    const anchor = document.createElement('a');\n","    anchor.href = new URL(path, url).toString();\n","    anchor.target = '_blank';\n","    anchor.setAttribute('data-href', url + path);\n","    anchor.textContent = text;\n","    element.appendChild(anchor);\n","  })(4050, \"/jobs/index.html\", \"https://localhost:4050/jobs/index.html\", window.element)"]},"metadata":{}}],"source":["# Instalar SDK java 8\n","!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n","\n","# Descargar Spark\n","!wget -q https://archive.apache.org/dist/spark/spark-3.3.4/spark-3.3.4-bin-hadoop3.tgz\n","\n","# Descomprimir la version de Spark\n","!tar xf spark-3.3.4-bin-hadoop3.tgz\n","\n","# Establecer las variables de entorno\n","import os\n","\n","os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n","os.environ[\"SPARK_HOME\"] = \"/content/spark-3.3.4-bin-hadoop3\"\n","\n","# Descargar findspark\n","!pip install -q findspark\n","\n","# Crear la sesión de Spark\n","import findspark\n","findspark.init()\n","from pyspark.sql import SparkSession\n","\n","spark = (\n","    SparkSession.builder\n","    .config('spark.ui.port', '4050')\n","    .getOrCreate()\n",")\n","\n","from google.colab import output\n","output.serve_kernel_port_as_window(4050, path='/jobs/index.html')\n","from pyspark.sql.functions import col\n","\n","spark.range(10000).toDF(\"id\").filter(col('id') / 2 == 0).write.mode('overwrite').parquet('/output')"]},{"cell_type":"markdown","metadata":{"id":"NYsFpayISwks"},"source":["## Funciones en Spark SQL"]},{"cell_type":"markdown","metadata":{"id":"hQWmggGTj9cD"},"source":[]},{"cell_type":"markdown","metadata":{"id":"Me9ESk0Pj-vT"},"source":["### Funciones de fecha y hora"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8nRvqo0oSx23"},"outputs":[],"source":["# Funciones de fecha y hora\n","\n","import findspark\n","findspark.init()\n","from pyspark.sql import SparkSession\n","\n","spark = SparkSession.builder.getOrCreate()\n","\n","data = spark.read.parquet('./data/convertir')\n","\n","data.printSchema()\n","\n","data.show(truncate=False)\n","\n","from pyspark.sql.functions import col, to_date, to_timestamp\n","\n","data1 = data.select(\n","    to_date(col('date')).alias('date1'),\n","    to_timestamp(col('timestamp')).alias('ts1'),\n","    to_date(col('date_str'), 'dd-MM-yyyy').alias('date2'),\n","    to_timestamp(col('ts_str'), 'dd-MM-yyyy mm:ss').alias('ts2')\n","\n",")\n","\n","data1.show(truncate=False)\n","\n","data1.printSchema()\n","\n","from pyspark.sql.functions import date_format\n","\n","data1.select(\n","    date_format(col('date1'), 'dd-MM-yyyy')\n",").show()\n","\n","df = spark.read.parquet('./data/calculo')\n","\n","df.show()\n","\n","from pyspark.sql.functions import datediff, months_between, last_day\n","\n","df.select(\n","    col('nombre'),\n","    datediff(col('fecha_salida'), col('fecha_ingreso')).alias('dias'),\n","    months_between(col('fecha_salida'), col('fecha_ingreso')).alias('meses'),\n","    last_day(col('fecha_salida')).alias('ultimo_dia_mes')\n",").show()\n","\n","from pyspark.sql.functions import date_add, date_sub\n","\n","df.select(\n","    col('nombre'),\n","    col('fecha_ingreso'),\n","    date_add(col('fecha_ingreso'), 14).alias('mas_14_dias'),\n","    date_sub(col('fecha_ingreso'), 1).alias('menos_1_dia')\n",").show()\n","\n","from pyspark.sql.functions import year, month, dayofmonth, dayofyear, hour, minute, second\n","\n","df.select(\n","    col('baja_sistema'),\n","    year(col('baja_sistema')),\n","    month(col('baja_sistema')),\n","    dayofmonth(col('baja_sistema')),\n","    dayofyear(col('baja_sistema')),\n","    hour(col('baja_sistema')),\n","    minute(col('baja_sistema')),\n","    second(col('baja_sistema'))\n",").show()"]},{"cell_type":"markdown","metadata":{"id":"b-_XCfLdj-OR"},"source":["### Funciones de trabajo con Strings"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZsF10m6aSytz"},"outputs":[],"source":["# Funciones para trabajo con strings\n","\n","import findspark\n","findspark.init()\n","from pyspark.sql import SparkSession\n","\n","spark = SparkSession.builder.getOrCreate()\n","\n","data = spark.read.parquet('./data/')\n","\n","data.show()\n","\n","from pyspark.sql.functions import ltrim, rtrim, trim\n","\n","data.select(\n","    ltrim('nombre').alias('ltrim'),\n","    rtrim('nombre').alias('rtrim'),\n","    trim('nombre').alias('trim')\n",").show()\n","\n","from pyspark.sql.functions import col, lpad, rpad\n","\n","data.select(\n","    trim(col('nombre')).alias('trim')\n",").select(\n","    lpad(col('trim'), 8, '-').alias('lpad'),\n","    rpad(col('trim'), 8, '=').alias('rpad')\n",").show()\n","\n","df1 = spark.createDataFrame([('Spark', 'es', 'maravilloso')], ['sujeto', 'verbo', 'adjetivo'])\n","\n","df1.show()\n","\n","from pyspark.sql.functions import concat_ws, lower, upper, initcap, reverse\n","\n","df1.select(\n","    concat_ws(' ', col('sujeto'), col('verbo'), col('adjetivo')).alias('frase')\n",").select(\n","    col('frase'),\n","    lower(col('frase')).alias('minuscula'),\n","    upper(col('frase')).alias('mayuscula'),\n","    initcap(col('frase')).alias('initcap'),\n","    reverse(col('frase')).alias('reversa')\n",").show()\n","\n","from pyspark.sql.functions import regexp_replace\n","\n","df2 = spark.createDataFrame([(' voy a casa por mis llaves',)], ['frase'])\n","\n","df2.show(truncate=False)\n","\n","df2.select(\n","    regexp_replace(col('frase'), 'voy|por', 'ir').alias('nueva_frase')\n",").show(truncate=False)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"HW-lQGZnkLLX"},"source":["### Funciones de trabajo con colecciones"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JZHKnPIESyqw"},"outputs":[],"source":["# Funciones para trabajo con colecciones\n","\n","import findspark\n","findspark.init()\n","from pyspark.sql import SparkSession\n","\n","spark = SparkSession.builder.getOrCreate()\n","\n","data = spark.read.parquet('./data/parquet/')\n","\n","data.show(truncate=False)\n","\n","data.printSchema()\n","\n","from pyspark.sql.functions import col, size, sort_array, array_contains\n","\n","data.select(\n","    size(col('tareas')).alias('tamaño'),\n","    sort_array(col('tareas')).alias('arreglo_ordenado'),\n","    array_contains(col('tareas'), 'buscar agua').alias('buscar_agua')\n",").show(truncate=False)\n","\n","from pyspark.sql.functions import explode\n","\n","data.select(\n","    col('dia'),\n","    explode(col('tareas')).alias('tareas')\n",").show()\n","\n","# Formato JSON\n","\n","json_df_str = spark.read.parquet('./data/JSON')\n","\n","json_df_str.show(truncate=False)\n","\n","json_df_str.printSchema()\n","\n","from pyspark.sql.types import StructType, StructField, StringType, ArrayType\n","\n","schema_json = StructType(\n","    [\n","     StructField('dia', StringType(), True),\n","     StructField('tareas', ArrayType(StringType()), True)\n","    ]\n",")\n","\n","from pyspark.sql.functions import from_json, to_json\n","\n","json_df = json_df_str.select(\n","    from_json(col('tareas_str'), schema_json).alias('por_hacer')\n",")\n","\n","json_df.printSchema()\n","\n","json_df.select(\n","    col('por_hacer').getItem('dia'),\n","    col('por_hacer').getItem('tareas'),\n","    col('por_hacer').getItem('tareas').getItem(0).alias('primer_tarea')\n",").show(truncate=False)\n","\n","json_df.select(\n","    to_json(col('por_hacer'))\n",").show(truncate=False)"]},{"cell_type":"markdown","metadata":{"id":"PMFx1J3DkOr3"},"source":["### Funciones when, coalesce y lit"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GgL2eHkuTGjR"},"outputs":[],"source":["# Funciones when, coalesce y lit\n","\n","import findspark\n","findspark.init()\n","from pyspark.sql import SparkSession\n","\n","spark = SparkSession.builder.getOrCreate()\n","\n","data = spark.read.parquet('./data/')\n","\n","data.show()\n","\n","from pyspark.sql.functions import col, when, lit, coalesce\n","\n","data.select(\n","    col('nombre'),\n","    when(col('pago') == 1, 'pagado').when(col('pago') == 2, 'sin pagar').otherwise('sin iniciar').alias('pago')\n",").show()\n","\n","data.select(\n","    coalesce(col('nombre'), lit('sin nombre')).alias('nombre')\n",").show()"]},{"cell_type":"markdown","metadata":{"id":"XXN17SfvkX3i"},"source":["### Funciones definidas por el usuario UDF"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"k8uBs6wtTGgb"},"outputs":[],"source":["# Funciones definidas por el usuario UDF\n","\n","import findspark\n","findspark.init()\n","from pyspark.sql import SparkSession\n","\n","spark = SparkSession.builder.getOrCreate()\n","\n","def cubo(n):\n","    return n * n * n\n","\n","from pyspark.sql.types import LongType\n","\n","spark.udf.register('cubo', f_cubo, LongType())\n","\n","spark.range(1,10).createOrReplaceTempView('df_temp')\n","\n","spark.sql(\"SELECT id, cubo(id) AS cubo FROM df_temp\").show()\n","\n","def bienvenida(nombre):\n","    return ('Hola {}'.format(nombre))\n","\n","from pyspark.sql.functions import udf\n","from pyspark.sql.types import StringType\n","\n","bienvenida_udf = udf(lambda x: bienvenida(x), StringType())\n","\n","df_nombre = spark.createDataFrame([('Jose',), ('Julia',)], ['nombre'])\n","\n","df_nombre.show()\n","\n","from pyspark.sql.functions import col\n","\n","df_nombre.select(\n","    col('nombre'),\n","    bienvenida_udf(col('nombre')).alias('bie_nombre')\n",").show()\n","\n","@udf(returnType=StringType())\n","def mayuscula(s):\n","    return s.upper()\n","\n","df_nombre.select(\n","    col('nombre'),\n","    mayuscula(col('nombre')).alias('may_nombre')\n",").show()\n","\n","import pandas as pd\n","\n","from pyspark.sql.functions import pandas_udf\n","\n","def cubo_pandas(a: pd.Series) -> pd.Series:\n","    return a * a * a\n","\n","cubo_udf = pandas_udf(cubo_pandas, returnType=LongType())\n","\n","x = pd.Series([1, 2, 3])\n","\n","print(cubo_pandas(x))\n","\n","df = spark.range(5)\n","\n","df.select(\n","    col('id'),\n","    cubo_udf(col('id')).alias('cubo_pandas')\n",").show()"]},{"cell_type":"markdown","metadata":{"id":"Stw0QEHRkdJ_"},"source":["### Funciones de ventana"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CxtJs-cSTGdy"},"outputs":[],"source":["# Funciones de ventana\n","\n","import findspark\n","findspark.init()\n","from pyspark.sql import SparkSession\n","\n","spark = SparkSession.builder.getOrCreate()\n","\n","df = spark.read.parquet('./data/')\n","\n","df.show()\n","\n","from pyspark.sql.window import Window\n","from pyspark.sql.functions import desc, row_number, rank, dense_rank, col\n","\n","windowSpec = Window.partitionBy('departamento').orderBy(desc('evaluacion'))\n","\n","# row_number\n","\n","df.withColumn('row_number', row_number().over(windowSpec)).filter(col('row_number').isin(1,2)).show()\n","\n","# rank\n","\n","df.withColumn('rank', rank().over(windowSpec)).show()\n","\n","# dense_rank\n","\n","df.withColumn('dense_rank', dense_rank().over(windowSpec)).show()\n","\n","# Agregaciones con especificaciones de ventana\n","\n","windowSpecAgg = Window.partitionBy('departamento')\n","\n","from pyspark.sql.functions import min, max, avg\n","\n","(df.withColumn('min', min(col('evaluacion')).over(windowSpecAgg))\n",".withColumn('max', max(col('evaluacion')).over(windowSpecAgg))\n",".withColumn('avg', avg(col('evaluacion')).over(windowSpecAgg))\n",".withColumn('row_number', row_number().over(windowSpec))\n"," ).show()"]},{"cell_type":"markdown","metadata":{"id":"f9PUeF-mkf9G"},"source":["### Catalyst Optimizer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OvwuWD4ASyok"},"outputs":[],"source":["# Catalyst Optimizer\n","\n","import findspark\n","findspark.init()\n","from pyspark.sql import SparkSession\n","\n","spark = SparkSession.builder.getOrCreate()\n","\n","data = spark.read.parquet('./data/')\n","\n","data.printSchema()\n","\n","data.show()\n","\n","from pyspark.sql.functions import col\n","\n","nuevo_df = (data.filter(col('MONTH').isin(6,7,8))\n","            .withColumn('dis_tiempo_aire', col('DISTANCE') / col('AIR_TIME'))\n",").select(\n","    col('AIRLINE'),\n","    col('dis_tiempo_aire')\n",").where(col('AIRLINE').isin('AA', 'DL', 'AS'))\n","\n","nuevo_df.explain(True)"]},{"cell_type":"markdown","metadata":{"id":"vTojhBfqkktY"},"source":["### Ejercicios"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Jl5LPyxtklJz"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"d93Xay9kA95I"},"source":["-----------\n","## Fin Notebook\n","-----------"]}],"metadata":{"colab":{"collapsed_sections":["_zJeZZqpJu73","LccNQX8WMBtl","Jz_8wAjF-CZL","q5eOVSiKAhWl","NYsFpayISwks","Me9ESk0Pj-vT","b-_XCfLdj-OR","HW-lQGZnkLLX","PMFx1J3DkOr3","XXN17SfvkX3i","Stw0QEHRkdJ_","f9PUeF-mkf9G"],"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}