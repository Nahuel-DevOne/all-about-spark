{"cells":[{"cell_type":"markdown","metadata":{"id":"IhqAQ_OOKTXl"},"source":["# **Spark SQL Avanzado**"]},{"cell_type":"markdown","metadata":{"id":"_zJeZZqpJu73"},"source":["## Introducción"]},{"cell_type":"markdown","metadata":{"id":"Ojkkv5J5rwg-"},"source":["### `Ventajas y desventajas de trabajar con Spark en Google Colab`"]},{"cell_type":"markdown","metadata":{"id":"AzJLfwssmyU6"},"source":["Ventajas:\n","- Fácil acceso\n","- Ejecutar Spark en prácticamente cualquier dispositivo, los recursos están en la nube.\n","- Como los recursos están la nube, no hay que preocuparse por los recursos de hardware\n","- Trabajo en equipo, más sencillo el trabajo colaborativo. Varias personas pueden trabajar sobre un mismo notebook.\n","\n","\n","Desventajas:\n","- No se guardan las configuraciones de Spark luego de un tiempo\n","> No obstante el notebook permanece intacto. Se puede volver a ejecutar las líneas de código para tener la configuración nuevamente.\n","- Escalabilidad, como el servicio es gratuito, los recursos son limitados.\n","> Para llevarlo a ambientes productivos, necesitamos una infraestructura capaz de brindarnos estas especificaciones."]},{"cell_type":"markdown","metadata":{"id":"CqjAFIHMJoQc"},"source":["## Instalaciones Necesarias para trabajar con Spark en Colab"]},{"cell_type":"markdown","metadata":{"id":"LccNQX8WMBtl"},"source":["### `Descarga e instalación de Apache Spark en Colab`\n","Se explica celda por celda las instalaciones necesarias. Para fines prácticos, utilizar la celda de abajo que instala todo junto."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JPNdekO7KHTp"},"outputs":[],"source":["# Instalar SDK Java 8\n","!apt-get install openjdk-8-jdk-headless -qq > /dev/null"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"peqw7BfRKjHE"},"outputs":[],"source":["# Descargar Spark 3.2.4\n","!wget -q https://archive.apache.org/dist/spark/spark-3.2.4/spark-3.2.4-bin-hadoop3.2.tgz"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wIsOwRkJKjEi"},"outputs":[],"source":["# Descomprimir el archivo descargado de Spark\n","!tar xf spark-3.2.4-bin-hadoop3.2.tgz"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sUtXLbwuKjBH"},"outputs":[],"source":["# Establecer las variables de entorno\n","import os\n","\n","os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n","os.environ[\"SPARK_HOME\"] = \"/content/spark-3.2.4-bin-hadoop3.2\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oMM5SJEPKi_X"},"outputs":[],"source":["# Instalar la librería findspark\n","!pip install -q findspark"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":36078,"status":"ok","timestamp":1708225550840,"user":{"displayName":"Nahuel Lopez","userId":"06859695819217714267"},"user_tz":180},"id":"51AZCLcVKi9x","outputId":"f9126ed1-b460-4e91-ea89-35223d84a511"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.9/316.9 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n"]}],"source":["# Instalar pyspark\n","!pip install -q pyspark"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2AMEAXrbKi8o"},"outputs":[],"source":["### verificar la instalación ###\n","import findspark\n","findspark.init()\n","\n","from pyspark.sql import SparkSession\n","spark = SparkSession.builder.master(\"local[*]\").getOrCreate()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6201,"status":"ok","timestamp":1708226374480,"user":{"displayName":"Nahuel Lopez","userId":"06859695819217714267"},"user_tz":180},"id":"QivCkbKRKi6g","outputId":"09e383c5-012f-421b-bb97-d329a12167b4"},"outputs":[{"name":"stdout","output_type":"stream","text":["+-----+\n","| Hola|\n","+-----+\n","|Mundo|\n","|Mundo|\n","|Mundo|\n","|Mundo|\n","|Mundo|\n","|Mundo|\n","|Mundo|\n","|Mundo|\n","|Mundo|\n","|Mundo|\n","+-----+\n","\n"]}],"source":["# Probando la sesión de Spark\n","df = spark.createDataFrame([{\"Hola\": \"Mundo\"} for x in range(10)])\n","# df.show(10, False)\n","df.show()"]},{"cell_type":"markdown","metadata":{"id":"U_xdI5-TOhbo"},"source":["### `Descarga e instalación de Apache Spark en una sola celda (Utilizar esta opción)`\n","Para fines prácticos, toda la instalación está en una celda, así luego de ejecutarse ya se puede trabajar con Spark."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":129065,"status":"ok","timestamp":1723487353125,"user":{"displayName":"Nahuel Lopez","userId":"06859695819217714267"},"user_tz":180},"id":"wEvprYN0Ot1l","outputId":"5cb64e13-1347-4f2b-d4c6-c7e3c9420b35"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.3/317.3 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n","+-----+\n","| Hola|\n","+-----+\n","|Mundo|\n","|Mundo|\n","|Mundo|\n","|Mundo|\n","|Mundo|\n","|Mundo|\n","|Mundo|\n","|Mundo|\n","|Mundo|\n","|Mundo|\n","+-----+\n","\n"]}],"source":["# Instalar SDK Java 8\n","!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n","\n","# Descargar Spark 3.2.4\n","!wget -q https://archive.apache.org/dist/spark/spark-3.2.4/spark-3.2.4-bin-hadoop3.2.tgz\n","\n","# Descomprimir el archivo descargado de Spark\n","!tar xf spark-3.2.4-bin-hadoop3.2.tgz\n","\n","# Establecer las variables de entorno\n","import os\n","\n","os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n","os.environ[\"SPARK_HOME\"] = \"/content/spark-3.2.4-bin-hadoop3.2\"\n","\n","# Instalar la librería findspark\n","!pip install -q findspark\n","\n","# Instalar pyspark\n","!pip install -q pyspark\n","\n","### verificar la instalación ###\n","import findspark\n","findspark.init()\n","\n","from pyspark.sql import SparkSession\n","spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n","\n","# Probando la sesión de Spark\n","df = spark.createDataFrame([{\"Hola\": \"Mundo\"} for x in range(10)])\n","# df.show(10, False)\n","df.show()"]},{"cell_type":"markdown","source":["## `Importante: Carga de archivos en Google Colab`\n","\n","Dos formas de cargar los archivos para resolver los ejercicios:\n","\n","1. **Montando el drive para acceder a los contenidos** de la unidad.\n","\n","  Utilizar esta opción si los datos para los ejercicios se cargan en una carpeta del drive y se quiere acceder a ella.\n","\n","2. **Utilizando el cuadro de archivos**, donde se carga el archivo que se quiere trabajar. Se guarda temporalmente.\n","> Esta es la forma que voy a estar utizando"],"metadata":{"id":"Jz_8wAjF-CZL"}},{"cell_type":"code","source":["# Levantar una sesión de Spark\n","import findspark\n","findspark.init()\n","from pyspark.sql import SparkSession\n","\n","spark = SparkSession.builder.appName('Cap2').master('local(*)').getOrCreate()\n","spark"],"metadata":{"id":"f-J8eWCz-Fei"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 1. Utilizando el montado al drive y yendo hacia la carpeta donde se encuentra el archivo."],"metadata":{"id":"QXy8YNeO-Jau"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"pmN9OdbH-KM9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["rdd_texto = sc.wholeTextFiles('/content/drive/MyDrive/Spark/data-ej-PySpark-RDD/el_valor_del_big_data.txt')\n","rdd_texto.collect()"],"metadata":{"id":"o79J5JOS-QN2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 2. Utilizando el cuadro de archivos, donde se carga el archivo que se quiere trabajar. Se guarda temporalmente.\n","\n","Esta es la que voy a estar utizando a lo largo de todos los notebooks.\n"],"metadata":{"id":"NAHsX9e2-Kxs"}},{"cell_type":"code","source":["rdd_texto = sc.wholeTextFiles('./el_valor_del_big_data.txt')\n","rdd_texto.collect()"],"metadata":{"id":"Lr2_B-R0-LeE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"q5eOVSiKAhWl"},"source":["## Spark UI en Colab"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"H1IQB2NmAleg","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1722544439913,"user_tz":180,"elapsed":49948,"user":{"displayName":"Nahuel Lopez","userId":"06859695819217714267"}},"outputId":"1dddf7c5-10f6-4930-bafe-4ccec733b3be"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["(async (port, path, text, element) => {\n","    if (!google.colab.kernel.accessAllowed) {\n","      return;\n","    }\n","    element.appendChild(document.createTextNode(''));\n","    const url = await google.colab.kernel.proxyPort(port);\n","    const anchor = document.createElement('a');\n","    anchor.href = new URL(path, url).toString();\n","    anchor.target = '_blank';\n","    anchor.setAttribute('data-href', url + path);\n","    anchor.textContent = text;\n","    element.appendChild(anchor);\n","  })(4050, \"/jobs/index.html\", \"https://localhost:4050/jobs/index.html\", window.element)"]},"metadata":{}}],"source":["# Instalar SDK java 8\n","!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n","\n","# Descargar Spark\n","!wget -q https://archive.apache.org/dist/spark/spark-3.3.4/spark-3.3.4-bin-hadoop3.tgz\n","\n","# Descomprimir la version de Spark\n","!tar xf spark-3.3.4-bin-hadoop3.tgz\n","\n","# Establecer las variables de entorno\n","import os\n","\n","os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n","os.environ[\"SPARK_HOME\"] = \"/content/spark-3.3.4-bin-hadoop3\"\n","\n","# Descargar findspark\n","!pip install -q findspark\n","\n","# Crear la sesión de Spark\n","import findspark\n","findspark.init()\n","from pyspark.sql import SparkSession\n","\n","spark = (\n","    SparkSession.builder\n","    .config('spark.ui.port', '4050')\n","    .getOrCreate()\n",")\n","\n","from google.colab import output\n","output.serve_kernel_port_as_window(4050, path='/jobs/index.html')\n","from pyspark.sql.functions import col\n","\n","spark.range(10000).toDF(\"id\").filter(col('id') / 2 == 0).write.mode('overwrite').parquet('/output')"]},{"cell_type":"markdown","metadata":{"id":"vw6VZXSpSNZA"},"source":["## Spark SQL Avanzado"]},{"cell_type":"markdown","metadata":{"id":"v-G812zxe7SN"},"source":[]},{"cell_type":"markdown","metadata":{"id":"KkpLftPxe3bN"},"source":["### Agregaciones"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"J8Pg82cqSUci"},"outputs":[],"source":["# Explorando los datos\n","\n","import findspark\n","findspark.init()\n","from pyspark.sql import SparkSession\n","\n","spark = SparkSession.builder.getOrCreate()\n","\n","df = spark.read.parquet('./data/')\n","\n","df.printSchema()\n","\n","df.show(20, truncate=False)\n"]},{"cell_type":"markdown","metadata":{"id":"0zJIVCCwe8Ba"},"source":["### Funciones"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"T3Z9A3yzSVUt"},"outputs":[],"source":["# Funciones count, countDistinct y approx_count_distinct\n","\n","import findspark\n","findspark.init()\n","from pyspark.sql import SparkSession\n","\n","spark = SparkSession.builder.getOrCreate()\n","\n","df = spark.read.parquet('./data/dataframe')\n","\n","df.printSchema()\n","\n","df.show()\n","\n","# count\n","\n","from pyspark.sql.functions import count\n","\n","df.select(\n","    count('nombre').alias('conteo_nombre'),\n","    count('color').alias('conteo_color')\n",").show()\n","\n","df.select(\n","    count('nombre').alias('conteo_nombre'),\n","    count('color').alias('conteo_color'),\n","    count('*').alias('conteo_general')\n",").show()\n","\n","# countDistinct\n","\n","from pyspark.sql.functions import countDistinct\n","\n","df.select(\n","    countDistinct('color').alias('colores_dif')\n",").show()\n","\n","# approx_count_distinct\n","\n","from pyspark.sql.functions import approx_count_distinct\n","\n","dataframe = spark.read.parquet('./data/vuelos')\n","\n","dataframe.printSchema()\n","\n","dataframe.select(\n","    countDistinct('AIRLINE'),\n","    approx_count_distinct('AIRLINE')\n",").show()\n"]},{"cell_type":"markdown","metadata":{"id":"g1z5xso0e90P"},"source":["### Funciones min y max"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s0B2ZOlCSVTB"},"outputs":[],"source":["# Funciones min y max\n","\n","import findspark\n","findspark.init()\n","from pyspark.sql import SparkSession\n","\n","spark = SparkSession.builder.getOrCreate()\n","\n","vuelos = spark.read.parquet('./data')\n","\n","vuelos.printSchema()\n","\n","from pyspark.sql.functions import min, max, col\n","\n","vuelos.select(\n","    min('AIR_TIME').alias('menor_timepo'),\n","    max('AIR_TIME').alias('mayor_tiempo')\n",").show()\n","\n","vuelos.select(\n","    min('AIRLINE_DELAY'),\n","    max('AIRLINE_DELAY')\n",").show()\n"]},{"cell_type":"markdown","metadata":{"id":"p4Qqkqe5fBhq"},"source":["### Funciones sum y sumDistinct"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"v1xQbT4kSVRG"},"outputs":[],"source":["# Funciones sum, sumDistinct y avg\n","\n","import findspark\n","findspark.init()\n","from pyspark.sql import SparkSession\n","\n","spark = SparkSession.builder.getOrCreate()\n","\n","vuelos = spark.read.parquet('./data/')\n","\n","from pyspark.sql.functions import sum, sumDistinct, avg, count\n","\n","# sum\n","\n","vuelos.printSchema()\n","\n","vuelos.select(\n","    sum('DISTANCE').alias('sum_dis')\n",").show()\n","\n","# sumDistinct\n","\n","vuelos.select(\n","    sumDistinct('DISTANCE').alias('sum_dis_dif')\n",").show()\n","\n","# avg\n","\n","vuelos.select(\n","    avg('AIR_TIME').alias('promedio_aire'),\n","    (sum('AIR_TIME') / count('AIR_TIME')).alias('prom_manual')\n",").show()\n"]},{"cell_type":"markdown","metadata":{"id":"sEqgOplGf6rT"},"source":["### Agregación con agrupación"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ToBOY8kQTIQt"},"outputs":[],"source":["# Agregación con agrupación\n","\n","import findspark\n","findspark.init()\n","from pyspark.sql import SparkSession\n","\n","spark = SparkSession.builder.getOrCreate()\n","\n","vuelos = spark.read.parquet('./data/')\n","\n","vuelos.printSchema()\n","\n","from pyspark.sql.functions import desc\n","\n","(vuelos.groupBy('ORIGIN_AIRPORT')\n","    .count()\n","    .orderBy(desc('count'))\n",").show()\n","\n","(vuelos.groupBy('ORIGIN_AIRPORT', 'DESTINATION_AIRPORT')\n","    .count()\n","    .orderBy(desc('count'))\n",").show()\n"]},{"cell_type":"markdown","metadata":{"id":"CWDte3lOf-KC"},"source":["### Varias agregaciones por grupo"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bSSxwk-hTIOY"},"outputs":[],"source":["# Varias agregaciones por grupo\n","\n","import findspark\n","findspark.init()\n","from pyspark.sql import SparkSession\n","\n","spark = SparkSession.builder.getOrCreate()\n","\n","vuelos = spark.read.parquet('./data/')\n","\n","from pyspark.sql.functions import count, min, max, desc, avg\n","\n","vuelos.groupBy('ORIGIN_AIRPORT').agg(\n","    count('AIR_TIME').alias('tiempo_aire'),\n","    min('AIR_TIME').alias('min'),\n","    max('AIR_TIME').alias('max')\n",").orderBy(desc('tiempo_aire')).show()\n","\n","vuelos.groupBy('MONTH').agg(\n","    count('ARRIVAL_DELAY').alias('conteo_de_retrasos'),\n","    avg('DISTANCE').alias('prom_dist')\n",").orderBy(desc('conteo_de_retrasos')).show()\n"]},{"cell_type":"markdown","metadata":{"id":"YiptC7kTgCM6"},"source":["### Agregación con pivote"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6YZx8tb1TIME"},"outputs":[],"source":["# Agregación con pivote\n","\n","import findspark\n","findspark.init()\n","from pyspark.sql import SparkSession\n","\n","spark = SparkSession.builder.getOrCreate()\n","\n","estudiantes = spark.read.parquet('./data/')\n","\n","estudiantes.show()\n","\n","from pyspark.sql.functions import min, max, avg, col\n","\n","estudiantes.groupBy('graduacion').pivot('sexo').agg(avg('peso')).show()\n","\n","estudiantes.groupBy('graduacion').pivot('sexo').agg(avg('peso'), min('peso'), max('peso')).show()\n","\n","estudiantes.groupBy('graduacion').pivot('sexo', ['M']).agg(avg('peso'), min('peso'), max('peso')).show()\n","\n","estudiantes.groupBy('graduacion').pivot('sexo', ['F']).agg(avg('peso'), min('peso'), max('peso')).show()\n"]},{"cell_type":"markdown","metadata":{"id":"cLpY1rmEgPet"},"source":["### Inner Join"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_waX_gIIgP3q"},"outputs":[],"source":["# Inner Join\n","\n","import findspark\n","findspark.init()\n","from pyspark.sql import SparkSession\n","\n","spark = SparkSession.builder.getOrCreate()\n","\n","empleados = spark.read.parquet('./data/empleados')\n","\n","departamentos = spark.read.parquet('./data/departamentos')\n","\n","empleados.show()\n","\n","departamentos.show()\n","\n","# Inner join\n","\n","from pyspark.sql.functions import col\n","\n","join_df = empleados.join(departamentos, col('num_dpto') == col('id'))\n","\n","join_df.show()\n","\n","join_df = empleados.join(departamentos, col('num_dpto') == col('id'), 'inner')\n","\n","join_df.show()\n","\n","join_df = empleados.join(departamentos).where(col('num_dpto') == col('id'))\n","\n","join_df.show()\n"]},{"cell_type":"markdown","metadata":{"id":"7KfvGnhbgQZy"},"source":["### Left Outer Join"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fT6syaongQuo"},"outputs":[],"source":["# Left Outer Join\n","\n","import findspark\n","findspark.init()\n","from pyspark.sql import SparkSession\n","\n","spark = SparkSession.builder.getOrCreate()\n","\n","empleados = spark.read.parquet('./data/empleados/')\n","\n","departamentos = spark.read.parquet('./data/departamentos/')\n","\n","from pyspark.sql.functions import col\n","\n","empleados.join(departamentos, col('num_dpto') == col('id'), 'leftouter').show()\n","\n","empleados.join(departamentos, col('num_dpto') == col('id'), 'left_outer').show()\n","\n","empleados.join(departamentos, col('num_dpto') == col('id'), 'left').show()\n"]},{"cell_type":"markdown","metadata":{"id":"yw4AEEhRgRJ8"},"source":["### Right Outer Join"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MPV0x9RKgRie"},"outputs":[],"source":["# Right Outer Join\n","\n","import findspark\n","findspark.init()\n","from pyspark.sql import SparkSession\n","\n","spark = SparkSession.builder.getOrCreate()\n","\n","empleados = spark.read.parquet('./data/empleados/')\n","\n","departamentos = spark.read.parquet('./data/departamentos/')\n","\n","from pyspark.sql.functions import col\n","\n","empleados.join(departamentos, col('num_dpto') == col('id'), 'rightouter').show()\n","\n","empleados.join(departamentos, col('num_dpto') == col('id'), 'right_outer').show()\n","\n","empleados.join(departamentos, col('num_dpto') == col('id'), 'right').show()\n"]},{"cell_type":"markdown","metadata":{"id":"_9ZcZL6XgfSK"},"source":["### Full Outer Join"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"S20OuGOfgfm9"},"outputs":[],"source":["# Full Outer Join\n","\n","import findspark\n","findspark.init()\n","from pyspark.sql import SparkSession\n","\n","spark = SparkSession.builder.getOrCreate()\n","\n","empleados = spark.read.parquet('./data/empleados/')\n","\n","departamentos = spark.read.parquet('./data/departamentos/')\n","\n","from pyspark.sql.functions import col\n","\n","empleados.join(departamentos, col('num_dpto') == col('id'), 'outer').show()\n"]},{"cell_type":"markdown","metadata":{"id":"8J-s9XH4ggHo"},"source":["### Left Anti Join"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mgjIJmo7gglZ"},"outputs":[],"source":["# Left Anti Join\n","\n","import findspark\n","findspark.init()\n","from pyspark.sql import SparkSession\n","\n","spark = SparkSession.builder.getOrCreate()\n","\n","empleados = spark.read.parquet('./data/empleados/')\n","\n","departamentos = spark.read.parquet('./data/departamentos/')\n","\n","from pyspark.sql.functions import col\n","\n","empleados.join(departamentos, col('num_dpto') == col('id'), 'left_anti').show()\n","\n","departamentos.join(empleados, col('num_dpto') == col('id'), 'left_anti').show()\n"]},{"cell_type":"markdown","metadata":{"id":"sEmAsVQvgiLr"},"source":["### Left Semi Join"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UcQ0Xsj1gjCc"},"outputs":[],"source":["# Left Semi Join\n","\n","import findspark\n","findspark.init()\n","from pyspark.sql import SparkSession\n","\n","spark = SparkSession.builder.getOrCreate()\n","\n","empleados = spark.read.parquet('./data/empleados/')\n","\n","departamentos = spark.read.parquet('./data/departamentos/')\n","\n","from pyspark.sql.functions import col\n","\n","empleados.join(departamentos, col('num_dpto') == col('id'), 'left_semi').show()\n"]},{"cell_type":"markdown","metadata":{"id":"vvfgFSC9gjao"},"source":["### Cross Join"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"di9GHBeFgjzD"},"outputs":[],"source":["# Cross Join\n","\n","import findspark\n","findspark.init()\n","from pyspark.sql import SparkSession\n","\n","spark = SparkSession.builder.getOrCreate()\n","\n","empleados = spark.read.parquet('./data/empleados/')\n","\n","departamentos = spark.read.parquet('./data/departamentos/')\n","\n","from pyspark.sql.functions import col\n","\n","df = empleados.crossJoin(departamentos)\n","\n","df.show()\n","\n","df.count()"]},{"cell_type":"markdown","metadata":{"id":"DBCbKPrZgkJh"},"source":["### Manejo de nombres de columnas duplicados"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FncyxGK5gk32"},"outputs":[],"source":["# Manejo de nombres de columnas duplicados\n","\n","import findspark\n","findspark.init()\n","from pyspark.sql import SparkSession\n","\n","spark = SparkSession.builder.getOrCreate()\n","\n","empleados = spark.read.parquet('./data/empleados/')\n","\n","departamentos = spark.read.parquet('./data/departamentos/')\n","\n","from pyspark.sql.functions import col\n","\n","depa = departamentos.withColumn('num_dpto', col('id'))\n","\n","depa.printSchema()\n","\n","empleados.printSchema()\n","\n","# Devuelve un error\n","\n","empleados.join(depa, col('num_dpto') == col('num_dpto'))\n","\n","# Forma correcta\n","\n","df_con_duplicados = empleados.join(depa, empleados['num_dpto'] == depa['num_dpto'])\n","\n","df_con_duplicados.printSchema()\n","\n","df_con_duplicados.select(empleados['num_dpto']).show()\n","\n","df2 = empleados.join(depa, 'num_dpto')\n","\n","df2.printSchema()\n","\n","empleados.join(depa, ['num_dpto']).printSchema()\n"]},{"cell_type":"markdown","metadata":{"id":"7-9rwntjglSm"},"source":["### Shuffle, hash join y broadcast hash join"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MM0cQYctglpe"},"outputs":[],"source":["# Shuffle Hash Join y Broadcast Hash Join\n","\n","import findspark\n","findspark.init()\n","from pyspark.sql import SparkSession\n","\n","spark = SparkSession.builder.getOrCreate()\n","\n","empleados = spark.read.parquet('./data/empleados/')\n","\n","departamentos = spark.read.parquet('./data/departamentos/')\n","\n","from pyspark.sql.functions import col, broadcast\n","\n","empleados.join(broadcast(departamentos), col('num_dpto') == col('id')).show()\n","\n","empleados.join(broadcast(departamentos), col('num_dpto') == col('id')).explain()"]},{"cell_type":"markdown","metadata":{"id":"oFfGOHPVgmP_"},"source":["### Ejercicios"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iqcG8TJcgmlh"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"d93Xay9kA95I"},"source":["-----------\n","## Fin Notebook\n","-----------"]}],"metadata":{"colab":{"collapsed_sections":["_zJeZZqpJu73","LccNQX8WMBtl","Jz_8wAjF-CZL","q5eOVSiKAhWl","vw6VZXSpSNZA"],"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}