{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["_zJeZZqpJu73","LccNQX8WMBtl","U_xdI5-TOhbo","IRp_mp9FSgVO","_-fsmm-uUd_N"],"authorship_tag":"ABX9TyPqYr3ftymBCYa73iMzf7Tl"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# **Spark con PySpark**"],"metadata":{"id":"IhqAQ_OOKTXl"}},{"cell_type":"markdown","source":["## Introducción"],"metadata":{"id":"_zJeZZqpJu73"}},{"cell_type":"markdown","source":["### `Ventajas y desventajas de trabajar con Spark en Google Colab`"],"metadata":{"id":"Ojkkv5J5rwg-"}},{"cell_type":"markdown","source":["Ventajas:\n","- Fácil acceso\n","- Ejecutar Spark en prácticamente cualquier dispositivo, los recursos están en la nube.\n","- Como los recursos están la nube, no hay que preocuparse por los recursos de hardware\n","- Trabajo en equipo, más sencillo el trabajo colaborativo. Varias personas pueden trabajar sobre un mismo notebook.\n","\n","\n","Desventajas:\n","- No se guardan las configuraciones de Spark luego de un tiempo\n","> No obstante el notebook permanece intacto. Se puede volver a ejecutar las líneas de código para tener la configuración nuevamente.\n","- Escalabilidad, como el servicio es gratuito, los recursos son limitados.\n","> Para llevarlo a ambientes productivos, necesitamos una infraestructura capaz de brindarnos estas especificaciones."],"metadata":{"id":"AzJLfwssmyU6"}},{"cell_type":"markdown","source":["## Instalaciones"],"metadata":{"id":"CqjAFIHMJoQc"}},{"cell_type":"markdown","source":["### `Descarga e instalación de Apache Spark en Colab`"],"metadata":{"id":"LccNQX8WMBtl"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"JPNdekO7KHTp"},"outputs":[],"source":["# Instalar SDK Java 8\n","!apt-get install openjdk-8-jdk-headless -qq > /dev/null"]},{"cell_type":"code","source":["# Descargar Spark 3.2.4\n","!wget -q https://archive.apache.org/dist/spark/spark-3.2.4/spark-3.2.4-bin-hadoop3.2.tgz"],"metadata":{"id":"peqw7BfRKjHE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Descomprimir el archivo descargado de Spark\n","!tar xf spark-3.2.4-bin-hadoop3.2.tgz"],"metadata":{"id":"wIsOwRkJKjEi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Establecer las variables de entorno\n","import os\n","\n","os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n","os.environ[\"SPARK_HOME\"] = \"/content/spark-3.2.4-bin-hadoop3.2\""],"metadata":{"id":"sUtXLbwuKjBH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Instalar la librería findspark\n","!pip install -q findspark"],"metadata":{"id":"oMM5SJEPKi_X"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Instalar pyspark\n","!pip install -q pyspark"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"51AZCLcVKi9x","executionInfo":{"status":"ok","timestamp":1708225550840,"user_tz":180,"elapsed":36078,"user":{"displayName":"Nahuel Lopez","userId":"06859695819217714267"}},"outputId":"f9126ed1-b460-4e91-ea89-35223d84a511"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.9/316.9 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n"]}]},{"cell_type":"code","source":["### verificar la instalación ###\n","import findspark\n","findspark.init()\n","\n","from pyspark.sql import SparkSession\n","spark = SparkSession.builder.master(\"local[*]\").getOrCreate()"],"metadata":{"id":"2AMEAXrbKi8o"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Probando la sesión de Spark\n","df = spark.createDataFrame([{\"Hola\": \"Mundo\"} for x in range(10)])\n","# df.show(10, False)\n","df.show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QivCkbKRKi6g","executionInfo":{"status":"ok","timestamp":1708226374480,"user_tz":180,"elapsed":6201,"user":{"displayName":"Nahuel Lopez","userId":"06859695819217714267"}},"outputId":"09e383c5-012f-421b-bb97-d329a12167b4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["+-----+\n","| Hola|\n","+-----+\n","|Mundo|\n","|Mundo|\n","|Mundo|\n","|Mundo|\n","|Mundo|\n","|Mundo|\n","|Mundo|\n","|Mundo|\n","|Mundo|\n","|Mundo|\n","+-----+\n","\n"]}]},{"cell_type":"markdown","source":["### `Descarga e instalación de Apache Spark en una sola celda`"],"metadata":{"id":"U_xdI5-TOhbo"}},{"cell_type":"code","source":["# Instalar SDK Java 8\n","!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n","\n","# Descargar Spark 3.2.4\n","!wget -q https://archive.apache.org/dist/spark/spark-3.2.4/spark-3.2.4-bin-hadoop3.2.tgz\n","\n","# Descomprimir el archivo descargado de Spark\n","!tar xf spark-3.2.4-bin-hadoop3.2.tgz\n","\n","# Establecer las variables de entorno\n","import os\n","\n","os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n","os.environ[\"SPARK_HOME\"] = \"/content/spark-3.2.4-bin-hadoop3.2\"\n","\n","# Instalar la librería findspark\n","!pip install -q findspark\n","\n","# Instalar pyspark\n","!pip install -q pyspark\n","\n","### verificar la instalación ###\n","import findspark\n","findspark.init()\n","\n","from pyspark.sql import SparkSession\n","spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n","\n","# Probando la sesión de Spark\n","df = spark.createDataFrame([{\"Hola\": \"Mundo\"} for x in range(10)])\n","# df.show(10, False)\n","df.show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wEvprYN0Ot1l","executionInfo":{"status":"ok","timestamp":1708226674655,"user_tz":180,"elapsed":28873,"user":{"displayName":"Nahuel Lopez","userId":"06859695819217714267"}},"outputId":"a03c5e4e-42a5-415f-925a-aae5e7c348fb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["+-----+\n","| Hola|\n","+-----+\n","|Mundo|\n","|Mundo|\n","|Mundo|\n","|Mundo|\n","|Mundo|\n","|Mundo|\n","|Mundo|\n","|Mundo|\n","|Mundo|\n","|Mundo|\n","+-----+\n","\n"]}]},{"cell_type":"markdown","source":["## Introducción a los RDD en Spark"],"metadata":{"id":"82tBFl3NQ1aT"}},{"cell_type":"markdown","source":["### Creación de una SparkSessión:\n","> SparkSession permite:\n","- Crear DataFrames\n","- Leer fuentes de datos\n","- Acceder a metadatos de catálogos\n","- Emitir consultas Spark SQL"],"metadata":{"id":"IRp_mp9FSgVO"}},{"cell_type":"code","source":["# Creando una sesión de Spark en pyspark\n","import findspark\n","findspark.init()\n","\n","from pyspark.sql import SparkSession\n","spark = SparkSession.builder.master(\"local[*]\").appName('Curso Pyspark').getOrCreate()\n","spark"],"metadata":{"id":"vmZ05qEHKi1s","colab":{"base_uri":"https://localhost:8080/","height":222},"executionInfo":{"status":"ok","timestamp":1708228269274,"user_tz":180,"elapsed":367,"user":{"displayName":"Nahuel Lopez","userId":"06859695819217714267"}},"outputId":"6e5d4bb4-a88c-4b56-8708-d163aed75ee0"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<pyspark.sql.session.SparkSession at 0x7f0d64290c10>"],"text/html":["\n","            <div>\n","                <p><b>SparkSession - in-memory</b></p>\n","                \n","        <div>\n","            <p><b>SparkContext</b></p>\n","\n","            <p><a href=\"http://640b857d1bff:4040\">Spark UI</a></p>\n","\n","            <dl>\n","              <dt>Version</dt>\n","                <dd><code>v3.2.4</code></dd>\n","              <dt>Master</dt>\n","                <dd><code>local[*]</code></dd>\n","              <dt>AppName</dt>\n","                <dd><code>pyspark-shell</code></dd>\n","            </dl>\n","        </div>\n","        \n","            </div>\n","        "]},"metadata":{},"execution_count":10}]},{"cell_type":"markdown","source":["### ¿Qué es una RDD? Resilient Distributed Dataset\n","Es la abstracción principal de Spark\n","Características:\n","- Dependencias: puede recrear un RDD a partir de estas.\n","- Particiones\n","- Función de cálculo"],"metadata":{"id":"_-fsmm-uUd_N"}},{"cell_type":"markdown","source":["#### Diferentes formas de crear un RDD"],"metadata":{"id":"P0yq6c9x5GN9"}},{"cell_type":"code","source":["# Diferentes formas de crear un RDD\n","\n","# Creando la sesión de Spark\n","import findspark\n","findspark.init()\n","\n","from pyspark.sql import SparkSession\n","spark = SparkSession.builder.getOrCreate()\n","\n","# creando el sparkContext\n","sc = spark.sparkContext\n","\n","# Crear un RDD vacío\n","rdd_vacio = sc.emptyRDD\n","\n","# Crear un RDD con parallelize\n","rdd_vacio3 = sc.parallelize([], 3)\n","rdd_vacio3.getNumPartitions()\n","rdd = sc.parallelize([1,2,3,4,5])\n","rdd.collect()\n","\n","# Crear un RDD desde un archivo de texto\n","rdd_texto = sc.textFile('./rdd_source.txt')\n","rdd_texto.collect()\n","rdd_texto_completo = sc.wholeTextFiles('./rdd_source.txt')\n","rdd_texto_completo.collect()\n","\n","# Crear un RDD desde otro RDD\n","rdd_suma = rdd.map(lambda x: x + 1)\n","rdd_suma.collect()\n","\n","# Crear un RDD desde un dataframe\n","df = spark.createDataFrame([(1, 'jose'), (2, 'juan')], ['id', 'nombre'])\n","\n","df.show()\n","\n","rdd_df = df.rdd\n","rdd_df.collect()"],"metadata":{"id":"YxaGoBsnQqNM","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1708228376425,"user_tz":180,"elapsed":1462,"user":{"displayName":"Nahuel Lopez","userId":"06859695819217714267"}},"outputId":"b5b8e737-048f-4aa8-b2fa-300fc1c79194"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["+---+------+\n","| id|nombre|\n","+---+------+\n","|  1|  jose|\n","|  2|  juan|\n","+---+------+\n","\n"]},{"output_type":"execute_result","data":{"text/plain":["[Row(id=1, nombre='jose'), Row(id=2, nombre='juan')]"]},"metadata":{},"execution_count":12}]},{"cell_type":"markdown","source":["### **Ejercicios**\n","\n","1) Cree una sesión de Spark con nombre Cap2 y asegúrese de que emplea todos los cores disponibles para ejecutar en su ambiente de trabajo.\n","\n","2) Cree dos RDD vacíos, uno de ellos no debe contener particiones y el otro debe tener 5 particiones. Utilice vías diferentes para crear cada RDD.\n","\n","3) Cree un RDD que contenga los números primos que hay entre 1 y 20.\n","\n","4) Cree un nuevo RDD a partir del RDD creado en el ejercicio anterior el cuál solo contenga los números primos mayores a 10.\n","\n","5) Descargue el archivo de texto adjunto a esta lección como recurso y guárdelo en una carpeta llamada data en el ambiente de trabajo de Colab.\n","\n","- a) Cree un RDD a partir de este archivo de texto en donde todo el documento esté contenido en un solo registro. ¿Cómo podría saber la dirección donde está guardado el archivo de texto a partir del RDD creado?\n","\n","- b) Si necesitara crear un RDD a partir del archivo de texto cargado previamente en donde cada línea del archivo fuera un registro del RDD, ¿cómo lo haría?"],"metadata":{"id":"jJqZHwYjLVWa"}},{"cell_type":"code","source":["# Ejercicio 1\n","import findspark\n","findspark.init()\n","\n","from pyspark.sql import SparkSession\n","\n","spark = SparkSession.builder.appName('Cap2').master('local(*)').getOrCreate()\n","\n","spark"],"metadata":{"id":"nm5UYLxWQqLC","colab":{"base_uri":"https://localhost:8080/","height":222},"executionInfo":{"status":"ok","timestamp":1708231401667,"user_tz":180,"elapsed":285,"user":{"displayName":"Nahuel Lopez","userId":"06859695819217714267"}},"outputId":"31d2c049-ba53-47a1-a522-6c1bf80cf61d"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<pyspark.sql.session.SparkSession at 0x7f0d64290c10>"],"text/html":["\n","            <div>\n","                <p><b>SparkSession - in-memory</b></p>\n","                \n","        <div>\n","            <p><b>SparkContext</b></p>\n","\n","            <p><a href=\"http://640b857d1bff:4040\">Spark UI</a></p>\n","\n","            <dl>\n","              <dt>Version</dt>\n","                <dd><code>v3.2.4</code></dd>\n","              <dt>Master</dt>\n","                <dd><code>local[*]</code></dd>\n","              <dt>AppName</dt>\n","                <dd><code>pyspark-shell</code></dd>\n","            </dl>\n","        </div>\n","        \n","            </div>\n","        "]},"metadata":{},"execution_count":16}]},{"cell_type":"code","source":["# Ejercicio 2\n","sc = spark.sparkContext\n","rdd1 = sc.emptyRDD()\n","rdd2 = sc.parallelize([], 5)"],"metadata":{"id":"pOIYcKV5QqJX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["rdd1.getNumPartitions()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1maAFEvOib3v","executionInfo":{"status":"ok","timestamp":1708231639594,"user_tz":180,"elapsed":268,"user":{"displayName":"Nahuel Lopez","userId":"06859695819217714267"}},"outputId":"a0f394ba-19d3-4df0-cc32-270e6c573685"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0"]},"metadata":{},"execution_count":19}]},{"cell_type":"code","source":["rdd2.getNumPartitions()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GLdTsq7HibqP","executionInfo":{"status":"ok","timestamp":1708231640704,"user_tz":180,"elapsed":6,"user":{"displayName":"Nahuel Lopez","userId":"06859695819217714267"}},"outputId":"c9e1ca10-5f38-41fe-c719-9f449ee83d56"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["5"]},"metadata":{},"execution_count":20}]},{"cell_type":"code","source":["# Ejercicio 3\n","rdd_primo = sc.parallelize([2,3,5,7,11,13,17,19])\n","rdd_primo"],"metadata":{"id":"cw70avp7QqHi","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1708231840041,"user_tz":180,"elapsed":316,"user":{"displayName":"Nahuel Lopez","userId":"06859695819217714267"}},"outputId":"6ece0de9-82e7-42f2-e921-804f272ee07f"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["ParallelCollectionRDD[42] at readRDDFromFile at PythonRDD.scala:274"]},"metadata":{},"execution_count":23}]},{"cell_type":"code","source":["# Ejercicio 4\n","rdd_primo.filter(lambda x: x > 10).collect()"],"metadata":{"id":"-EltwJmnQqFt","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1708231898636,"user_tz":180,"elapsed":295,"user":{"displayName":"Nahuel Lopez","userId":"06859695819217714267"}},"outputId":"d78f4152-99bc-4a5b-8149-b458812b9fa7"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[11, 13, 17, 19]"]},"metadata":{},"execution_count":24}]},{"cell_type":"code","source":["# Ejercicio 5\n","# a)\n","rdd_texto = sc.wholeTextFiles('./data/el_valor_del_big_data.txt')\n","rdd_texto.collect()"],"metadata":{"id":"baSeewoqQqD8","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1708232111592,"user_tz":180,"elapsed":275,"user":{"displayName":"Nahuel Lopez","userId":"06859695819217714267"}},"outputId":"3e2f5138-6ffc-4fef-90ef-541b93b8c3b5"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('file:/content/data/el_valor_del_big_data.txt',\n","  'El valor y la realidad de big data\\r\\n\\r\\nEn los últimos años, han surgido otras \"dos V\": valor y veracidad. Los datos poseen un valor intrínseco. Sin embargo, no tienen ninguna utilidad hasta que dicho valor se descubre. Resulta igualmente importante: ¿cuál es la veracidad de sus datos y cuánto puede confiar en ellos?\\r\\n\\r\\nHoy en día, el big data se ha convertido en un activo crucial. Piense en algunas de las mayores empresas tecnológicas del mundo. Gran parte del valor que ofrecen procede de sus datos, que analizan constantemente para generar una mayor eficiencia y desarrollar nuevos productos.\\r\\n\\r\\nAvances tecnológicos recientes han reducido exponencialmente el coste del almacenamiento y la computación de datos, haciendo que almacenar datos resulte más fácil y barato que nunca. Actualmente, con un mayor volumen de big data más barato y accesible, puede tomar decisiones empresariales más acertadas y precisas.\\r\\n\\r\\nIdentificar el valor del big data no pasa solo por analizarlo (que es ya una ventaja en sí misma). Se trata de todo un proceso de descubrimiento que requiere que los analistas, usuarios empresariales y ejecutivos se planteen las preguntas correctas, identifiquen patrones, formulen hipótesis informadas y predigan comportamientos.')]"]},"metadata":{},"execution_count":25}]},{"cell_type":"code","source":["# b)\n","rdd_texto1= sc.textFile('./data/el_valor_del_big_data.txt')\n","rdd_texto1.collect()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"X8utzp9ZkuKS","executionInfo":{"status":"ok","timestamp":1708232308857,"user_tz":180,"elapsed":462,"user":{"displayName":"Nahuel Lopez","userId":"06859695819217714267"}},"outputId":"509b8d8b-ede7-44a5-8de3-8e388ce41e61"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['El valor y la realidad de big data',\n"," '',\n"," 'En los últimos años, han surgido otras \"dos V\": valor y veracidad. Los datos poseen un valor intrínseco. Sin embargo, no tienen ninguna utilidad hasta que dicho valor se descubre. Resulta igualmente importante: ¿cuál es la veracidad de sus datos y cuánto puede confiar en ellos?',\n"," '',\n"," 'Hoy en día, el big data se ha convertido en un activo crucial. Piense en algunas de las mayores empresas tecnológicas del mundo. Gran parte del valor que ofrecen procede de sus datos, que analizan constantemente para generar una mayor eficiencia y desarrollar nuevos productos.',\n"," '',\n"," 'Avances tecnológicos recientes han reducido exponencialmente el coste del almacenamiento y la computación de datos, haciendo que almacenar datos resulte más fácil y barato que nunca. Actualmente, con un mayor volumen de big data más barato y accesible, puede tomar decisiones empresariales más acertadas y precisas.',\n"," '',\n"," 'Identificar el valor del big data no pasa solo por analizarlo (que es ya una ventaja en sí misma). Se trata de todo un proceso de descubrimiento que requiere que los analistas, usuarios empresariales y ejecutivos se planteen las preguntas correctas, identifiquen patrones, formulen hipótesis informadas y predigan comportamientos.']"]},"metadata":{},"execution_count":26}]},{"cell_type":"code","source":[],"metadata":{"id":"Y0vsG88hQqBw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"9ml4vHm4Qp-_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Transformaciones en un RDD"],"metadata":{"id":"gljR-9AYRWDd"}},{"cell_type":"markdown","source":["### Los RDD son inmutables y cada operación crea un nuevo RDD.\n","\n","Principales operaciones:\n","- *`Transformaciones`*:\n","crean un nuevo RDD, como dividir el elemento de entrada, filtrar elementos, realizar cálculos de algún tipo, etc.\n","  - Transformaciones generales: map, filter, flatMap, groupByKey, sortByKey, combineByKey, etc.\n","  - Transformaciones matemáticas o estadísticas: sampleByKey, randomSplit, etc.\n","  - Transformaciones de conjunto o relacionales: cogroup, join, subtractByKey, fullOuterJoin, leftOuterJoin, rightOuterJoin, etc.\n","  - Transformaciones basadas en estructuras de datos: partitionBy, repartition, zipWithIndex, coalesce, etc.\n","\n","- *`Acciones`*\n","\n","> Posee Lazy Evaluation"],"metadata":{"id":"OGPspWQrXx14"}},{"cell_type":"markdown","source":["### map"],"metadata":{"id":"-5Dw2QtR6Js_"}},{"cell_type":"code","source":["# Transformaciones: función map\n","\n","import findspark\n","findspark.init()\n","from pyspark.sql import SparkSession\n","\n","spark = SparkSession.builder.getOrCreate()\n","\n","sc = spark.sparkContext\n","\n","rdd = sc.parallelize([1,2,3,4,5])\n","\n","rdd_resta = rdd.map(lambda x: x - 1)\n","rdd_resta.collect()\n","\n","rdd_par = rdd.map(lambda x: x % 2 == 0)\n","\n","rdd_par.collect()\n","\n","rdd_texto = sc.parallelize(['jose', 'juan', 'lucia'])\n","\n","rdd_mayuscula = rdd_texto.map(lambda x: x.upper())\n","\n","rdd_mayuscula.collect()\n","\n","rdd_hola = rdd_texto.map(lambda x: 'Hola ' + x)\n","\n","rdd_hola.collect()"],"metadata":{"id":"G-sGZwR8Qp7L"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### flatMap"],"metadata":{"id":"IiaO13iG6KIM"}},{"cell_type":"code","source":["# Transformaciones: función flatMap\n","\n","import findspark\n","findspark.init()\n","from pyspark.sql import SparkSession\n","\n","spark = SparkSession.builder.getOrCreate()\n","\n","sc = spark.sparkContext\n","\n","rdd = sc.parallelize([1,2,3,4,5])\n","\n","rdd_cuadrado = rdd.map(lambda x: (x, x ** 2))\n","\n","rdd_cuadrado.collect()\n","\n","rdd_cuadrado_flat = rdd.flatMap(lambda x: (x, x ** 2))\n","\n","rdd_cuadrado_flat.collect()\n","\n","rdd_texto = sc.parallelize(['jose', 'juan', 'lucia'])\n","\n","rdd_mayuscula = rdd_texto.flatMap(lambda x: (x, x.upper()))\n","\n","rdd_mayuscula.collect()"],"metadata":{"id":"GK8L3RfCQp5R"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### filter"],"metadata":{"id":"SVR2RpMn6X6u"}},{"cell_type":"code","source":["# Transformaciones: función filter\n","\n","import findspark\n","findspark.init()\n","from pyspark.sql import SparkSession\n","\n","spark = SparkSession.builder.getOrCreate()\n","sc = spark.sparkContext\n","\n","rdd = sc.parallelize([1,2,3,4,5,6,7,8,9])\n","\n","rdd_par = rdd.filter(lambda x: x % 2 == 0)\n","\n","rdd_par.collect()\n","\n","rdd_impar = rdd.filter(lambda x: x % 2 != 0)\n","\n","rdd_impar.collect()\n","\n","rdd_texto = sc.parallelize(['jose', 'juaquin', 'juan', 'lucia', 'karla', 'katia'])\n","\n","rdd_k = rdd_texto.filter(lambda x: x.startswith('k'))\n","\n","rdd_k.collect()\n","\n","rdd_filtro = rdd_texto.filter(lambda x: x.startswith('j') and x.find('u') == 1)\n","\n","rdd_filtro.collect()"],"metadata":{"id":"cjwaLYgRRa5B"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### coalesce"],"metadata":{"id":"2-WRgJTH6aRJ"}},{"cell_type":"code","source":["# Transformaciones: función coalesce\n","\n","import findspark\n","findspark.init()\n","from pyspark.sql import SparkSession\n","\n","spark = SparkSession.builder.getOrCreate()\n","sc = spark.sparkContext\n","\n","rdd = sc.parallelize([1,2,3.4,5], 10)\n","\n","rdd.getNumPartitions()\n","\n","rdd.coalesce(5)\n","rdd.getNumPartitions()\n","\n","rdd5 = rdd.coalesce(5)\n","\n","rdd5.getNumPartitions()"],"metadata":{"id":"v6N2OUzNRa1J"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### repartition"],"metadata":{"id":"2OxYZse-6c8R"}},{"cell_type":"code","source":["# Transformaciones: función repartition\n","\n","import findspark\n","findspark.init()\n","from pyspark.sql import SparkSession\n","\n","spark = SparkSession.builder.getOrCreate()\n","sc = spark.sparkContext\n","\n","rdd = sc.parallelize([1,2,3,4,5], 3)\n","\n","rdd.getNumPartitions()\n","\n","rdd7 = rdd.repartition(7)\n","\n","rdd7.getNumPartitions()"],"metadata":{"id":"5R3gyuuuRazD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### reduceByKey"],"metadata":{"id":"gT5vaEP06fqQ"}},{"cell_type":"code","source":["# Transformaciones: función reduceByKey\n","\n","import findspark\n","findspark.init()\n","from pyspark.sql import SparkSession\n","\n","spark = SparkSession.builder.getOrCreate()\n","sc = spark.sparkContext\n","\n","rdd = sc.parallelize(\n","    [('casa', 2),\n","     ('parque', 1),\n","     ('que', 5),\n","     ('casa', 1),\n","     ('escuela', 2),\n","     ('casa', 1),\n","     ('que', 1)]\n",")\n","\n","rdd.collect()\n","\n","rdd_reduciodo = rdd.reduceByKey(lambda x,y: x + y)\n","\n","rdd_reduciodo.collect()"],"metadata":{"id":"2qXI1OJ5Rawr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Ejercicios"],"metadata":{"id":"vsz7rhRT6pBY"}},{"cell_type":"code","source":["# Transacciones\n","# (1001, 52.3)\n","# (1005, 20.8)\n","# (1001, 10.1)\n","# (1004, 52.7)\n","# (1005, 20.7)\n","# (1002, 85.3)\n","# (1004, 20.9)\n"],"metadata":{"id":"aFD47WHtRauD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"dMZVxOOhRai-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Acciones sobre un RDD en Spark"],"metadata":{"id":"ivqELxkeRbPD"}},{"cell_type":"markdown","source":["### Lazy evaluation\n","\n","rdd original\n","- Filtrar azules\n","- Filtrar rojos\n","- Mostrar el número de registros --> Acción\n","- Tomar sólo los negros\n","- Mostrar el número de registros --> Acción\n","\n","### Tipos de acciones\n","Dos tipos:\n","- Driver\n","- Distributed"],"metadata":{"id":"C0Knn6cr7ArR"}},{"cell_type":"markdown","source":["### reduce"],"metadata":{"id":"Rj2Ejpol65JW"}},{"cell_type":"code","source":["# Acciones: función reduce\n","\n","import findspark\n","findspark.init()\n","from pyspark.sql import SparkSession\n","\n","spark = SparkSession.builder.getOrCreate()\n","sc = spark.sparkContext\n","\n","rdd = sc.parallelize([2,4,6,8])\n","\n","rdd.reduce(lambda x,y: x + y)\n","\n","rdd1 = sc.parallelize([1,2,3,4])\n","\n","rdd1.reduce(lambda x,y: x * y)"],"metadata":{"id":"TZSK3nIWRxuZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### count"],"metadata":{"id":"67Piwtx87Caz"}},{"cell_type":"code","source":["# Acciones: función count\n","\n","import findspark\n","findspark.init()\n","from pyspark.sql import SparkSession\n","\n","spark = SparkSession.builder.getOrCreate()\n","sc = spark.sparkContext\n","\n","rdd = sc.parallelize(['j', 'o', 's', 'e'])\n","\n","rdd.count()\n","\n","rdd1 = sc.parallelize([item for item in range(10)])\n","\n","rdd1.count()"],"metadata":{"id":"EBqQM6kmRyvW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### collect"],"metadata":{"id":"eaCeTraM7EUY"}},{"cell_type":"code","source":["# Acciones: función collect\n","\n","import findspark\n","findspark.init()\n","from pyspark.sql import SparkSession\n","\n","spark = SparkSession.builder.getOrCreate()\n","sc = spark.sparkContext\n","\n","rdd = sc.parallelize('Hola Apache Spark!'.split(' '))\n","\n","rdd.collect()\n","\n","rdd1 = sc.parallelize([(item, item ** 2) for item in range(20)])\n","\n","rdd1.collect()\n","\n"],"metadata":{"id":"Iz1xBuKERyta"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### take, max y saveAsTextFile"],"metadata":{"id":"d6ascoSt7GGn"}},{"cell_type":"code","source":["# Acciones: funciones take, max y saveAsTextFile\n","\n","import findspark\n","findspark.init()\n","from pyspark.sql import SparkSession\n","\n","spark = SparkSession.builder.getOrCreate()\n","sc = spark.sparkContext\n","\n","# take\n","\n","rdd = sc.parallelize('La programación es bella'.split(' '))\n","\n","rdd.take(2)\n","\n","rdd.take(4)\n","\n","# max\n","\n","rdd1 = sc.parallelize([item/(item + 1) for item in range(10)])\n","\n","rdd1.max()\n","\n","rdd1.collect()\n","\n","# saveAsTextFile\n","\n","rdd.collect()\n","\n","rdd.saveAsTextFile('./rdd')\n","\n","rdd.coalesce(1).saveAsTextFile('./rdd1')"],"metadata":{"id":"HMiVdwvnRyrn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Ejercicios"],"metadata":{"id":"3Ba0HQzV7Q5h"}},{"cell_type":"code","source":[],"metadata":{"id":"BtxWxtfeRyp-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"rfHALPv2Ryow"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"NH-D_Ve6Rym0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Aspectos avanzados sobre RDD"],"metadata":{"id":"3M2g-CDxRzN6"}},{"cell_type":"markdown","source":[],"metadata":{"id":"E7gdb8CQ7avB"}},{"cell_type":"markdown","source":["### Almacenamiento y cache"],"metadata":{"id":"wLW2aOn87b3x"}},{"cell_type":"markdown","source":["El almacenamiento en caché permite que Spark conserve los datos en todos los cálculos y operaciones. Almacena el RDD, tanto como sea posible, en la memoria. Si los datos que se solicitan para almacenar en caché son más grandes que la memoria disponible, el rendimiento disminuirá, porque se utilizará disco en lugar de memoria.\n","Podemos marcar un RDD como almacenado en caché usando persist() o cache()\n","cache() es simplemente un sinónimo de persist(MEMORY_ONLY).\n","persist() puede usar memoria o disco o ambos.\n","\n","Valores posibles para el nivel de almacenamiento\n","\n","| Nivel de Almacenamiento | Significado |\n","|----------|--------------------|\n","| MEMORY_ONLY  | Almacena RDD como objetos Java deserializados en la JVM. Si el RDD no cabe en la memoria, algunas particiones no se almacenarán en caché y se volverán a calcular sobre la marcha cada vez que se necesiten. Este el nivel por defecto. |\n","| MEMORY_AND_DISK  | Almacena RDD como objetos Java deserializados en la JVM. Si el RDD no cabe en la memoria, almacena las particiones que no quepan en el disco y las lee desde allí cuando sea necesario.   |\n","| DISK_ONLY  | Almacene las particiones RDD solo en el disco. |\n","| MEMORY_ONLY_2,MEMORY_AND_DISK_2, etc | Igual que los niveles anteriores, pero replica cada partición en dos nodos del clúster.   |\n","\n","El nivel de almacenamiento a elegir depende de la situación\n","- Si los RDD caben en la memoria, usar MEMORY_ONLY ya que es la opción más rápida para el rendimiento de ejecución.\n","- DISK_ONLY no debe usarse a menos que sus cálculos sean costosos.\n","- Utilizar almacenamiento replicado para una mejor tolerancia a fallas si se puede ahorrar la memoria adicional necesaria. Esto evitará que se vuelvan a calcular las particiones perdidas para obtener la mejor disponibilidad.\n","- Se puede utilizar la función persist() para liberar el contenido en caché.\n","\n","\n"],"metadata":{"id":"CfHemOPovo1_"}},{"cell_type":"code","source":["# Almacenamiento en caché\n","\n","import findspark\n","findspark.init()\n","from pyspark.sql import SparkSession\n","\n","spark = SparkSession.builder.getOrCreate()\n","sc = spark.sparkContext\n","\n","rdd = sc.parallelize([item for item in range(10)])\n","\n","from pyspark.storagelevel import StorageLevel\n","\n","rdd.persist(StorageLevel.MEMORY_ONLY)\n","\n","rdd.unpersist()\n","\n","rdd.persist(StorageLevel.DISK_ONLY)\n","\n","rdd.unpersist()\n","\n","rdd.cache()"],"metadata":{"id":"kL2HtzAASFiY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Particionado"],"metadata":{"id":"vI06F6YR7gIX"}},{"cell_type":"code","source":["# HashPartitioner\n","\n","import findspark\n","findspark.init()\n","from pyspark.sql import SparkSession\n","\n","spark = SparkSession.builder.getOrCreate()\n","sc = spark.sparkContext\n","\n","rdd = sc.parallelize(['x', 'y', 'z'])\n","\n","hola = 'Hola'\n","\n","hash(hola)\n","\n","num_particiones = 6\n","\n","# indice = hash(item) % num_particiones\n","\n","hash('x') % num_particiones\n","\n","hash('y') % num_particiones\n","\n","hash('z') % num_particiones\n","\n"],"metadata":{"id":"iUNgzl2lSGRe"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Broadcast y variables"],"metadata":{"id":"-JH4_7Cu7h_w"}},{"cell_type":"code","source":["# Broadcast variables\n","\n","import findspark\n","findspark.init()\n","from pyspark.sql import SparkSession\n","\n","spark = SparkSession.builder.getOrCreate()\n","sc = spark.sparkContext\n","\n","rdd = sc.parallelize([item for item in range(10)])\n","\n","uno = 1\n","\n","br_uno = sc.broadcast(uno)\n","\n","rdd1 = rdd.map(lambda x: x + br_uno.value)\n","\n","rdd1.collect()\n","\n","br_uno.unpersist()\n","\n","rdd1  = rdd.map(lambda x: x + br_uno.value)\n","\n","rdd1.collect()\n","\n","br_uno.destroy()\n","\n","rdd1  = rdd.map(lambda x: x + br_uno.value)\n","\n","rdd1.take(5)\n","\n"],"metadata":{"id":"pgAGQU4zSGPV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Acumuladores"],"metadata":{"id":"XnnLlQkM7mvt"}},{"cell_type":"code","source":["# Acumuladores\n","\n","import findspark\n","findspark.init()\n","from pyspark.sql import SparkSession\n","\n","spark = SparkSession.builder.getOrCreate()\n","sc = spark.sparkContext\n","\n","acumulador = sc.accumulator(0)\n","\n","rdd = sc.parallelize([2,4,6,8,10])\n","\n","rdd.foreach(lambda x: acumulador.add(x))\n","\n","print(acumulador.value)\n","\n","rdd1 = sc.parallelize('Mi nombre es Jose Miguel y me siento genial'.split(' '))\n","\n","acumulador1 = sc.accumulator(0)\n","\n","rdd1.foreach(lambda x: acumulador1.add(1))\n","\n","print(acumulador1.value)"],"metadata":{"id":"cdQKeZwrSGNX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Ejercicios"],"metadata":{"id":"MmvbRGwE7o3n"}},{"cell_type":"code","source":[],"metadata":{"id":"rEYs98JGSGFx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"Hu1wvpmfTJvb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"KjhYApimTJs5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["-----------------------------------------------------------------------------------"],"metadata":{"id":"Pg1iRb-IS-nq"}}]}